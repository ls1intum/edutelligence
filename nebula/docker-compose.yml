version: "3.9"

services:
  transcriber:
    build:
      context: .
      dockerfile: docker/transcriber/Dockerfile
    container_name: nebula-transcriber
    expose:
      - "5000"  # interne Kommunikation mit Envoy
    environment:
      - APPLICATION_YML_PATH=/app/application_local.nebula.yml
      - LLM_CONFIG_PATH=/app/llm_config.nebula.yml
    volumes:
      - ./application_local.nebula.yml:/app/application_local.nebula.yml:ro
      - ./llm_config.nebula.yml:/app/llm_config.nebula.yml:ro
      - ./temp:/app/temp
    restart: unless-stopped

  faq:
    build:
      context: .
      dockerfile: docker/faq/Dockerfile
    container_name: ${FAQ_SERVICE_NAME}
    expose:
      - "${FAQ_SERVICE_PORT}"
    environment:
      - APPLICATION_YML_PATH=/app/application_local.nebula.yml
      - LLM_CONFIG_PATH=/app/llm_config.nebula.yml
      - FAQ_SERVICE_PORT=${FAQ_SERVICE_PORT}
    volumes:
      - ./application_local.nebula.yml:/app/application_local.nebula.yml:ro
      - ./llm_config.nebula.yml:/app/llm_config.nebula.yml:ro
    restart: unless-stopped

  envoy:
    image: envoyproxy/envoy:v1.30-latest
    container_name: envoy
    volumes:
      - ./envoy.yaml:/etc/envoy/envoy.yaml:ro
    ports:
      - "8000:8000"     # HTTP for FastAPI
      - "50051:50051"   # gRPC
      -  "9901:9901"     # Admin interface
    depends_on:
      - transcriber
      - faq
