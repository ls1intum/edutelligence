# Pyris V3

Pyris is an intermediary system that connects the [Artemis](https://github.com/ls1intum/Artemis) platform with various
Large Language Models (LLMs). It provides a REST API that allows Artemis to interact with different pipelines based on
specific tasks.

Currently, Pyris powers [Iris](https://artemis.cit.tum.de/about-iris), a virtual AI tutor that assists students with
their programming exercises on Artemis in a pedagogically meaningful way.

## Table of Contents

- [Pyris V2](#pyris-v2)
  - [Table of Contents](#table-of-contents)
  - [Features](#features)
  - [Setup](#setup)
    - [Prerequisites](#prerequisites)
    - [Local Development Setup](#local-development-setup)
      - [Steps](#steps)
      - [Testing](#testing)
      - [Linting](#linting)
      - [Detect Secrets](#detect-secrets)
    - [Docker Setup](#docker-setup)
      - [Prerequisites](#prerequisites-1)
      - [Docker Compose Files](#docker-compose-files)
      - [Running the Containers](#running-the-containers)
        - [**Development Environment**](#development-environment)
        - [**Production Environment**](#production-environment)
          - [**Option 1: With Nginx**](#option-1-with-nginx)
          - [**Option 2: Without Nginx**](#option-2-without-nginx)
      - [Managing the Containers](#managing-the-containers)
      - [Customizing Configuration](#customizing-configuration)
  - [Troubleshooting](#troubleshooting)

## Features

- **Exercise Support**: Empowers Iris to provide feedback on programming exercises, enhancing the learning experience
  for students. Iris analyzes submitted code, feedback, and build logs generated by Artemis to provide detailed
  insights.

- **Course Content Support**: Leverages RAG (Retrieval-Augmented Generation) to enable Iris to provide detailed
  explanations for course content, making it easier for students to understand complex topics based on
  instructor-provided learning materials.

- **Competency Generation**: Automates the generation of competencies for courses, reducing manual effort in creating
  Artemis competencies.

## Setup

### Prerequisites

- **Python 3.12**: Ensure that Python 3.12 is installed.

  ```bash
  python --version
  ```

- **Docker and Docker Compose**: Required for containerized deployment.

---

### Local Development Setup

> **Note:** If you need to modify the local Weaviate vector database setup, please refer to
> the [Weaviate Documentation](https://weaviate.io/developers/weaviate/quickstart).

#### Steps

1. **Clone the Edutelligence Repository**

   Clone the Edutelligence repository into a directory on your machine:

   ```bash
   git clone https://github.com/ls1intum/edutelligence.git Edutelligence
   ```

   Then navigate to the `iris` subdirectory and continue with the next steps.

2. **Install Dependencies**

   Iris uses [Poetry](https://python-poetry.org/) to manage the dependencies and virtual environment.
   To setup the virtual environment and install the dependencies, run the following command:

   ```bash
   poetry install
   ```

3. **Setup pre-commit**

   The repository also uses [pre-commit](https://pre-commit.com/) to enforce code style and quality using a variety of
   tools.
   To install the pre-commit hooks, run the following command **in the edutelligence repository root`**:

   ```bash
   pre-commit install
   ```

4. **Setup your IDE**

   If you use an IDE like PyCharm, you'll want to setup your interpreter to use the Poetry environment.
   For PyCharm you can do this by following these steps:
   1. Open the `iris` folder as a new PyCharm project
   2. Go to `File > Settings > Project: MemIris > Python Interpreter`
   3. Click the gear icon and select `Add...`
   4. Select `Poetry Environment` and choose the `poetry` executable
   5. Click `OK` and `Apply`
   6. PyCharm should now use the Poetry environment for the project

   You should also configure the IDE to use the correct source directories.
   Not doing this may mess up the imports your IDE automatically adds in the code.
   In PyCharm, you can do this like this:
   1. Right-click on the `src` folder in the project view
   2. Select `Mark Directory as > Sources Root`
   3. This will tell PyCharm to use the `src` folder as the source root
   4. Right-click on the `tests` folder in the project view
   5. Select `Mark Directory as > Test Sources Root`
   6. This will tell PyCharm to use the `tests` folder as the test source root
   7. Now select `File > Invalidate Caches...` and click `Invalidate and Restart`

5. **Create Configuration Files**
   - **Create an Application Configuration File**

     Create an `application.local.yml` file in the root directory. You can use the provided `application.example.yml`
     as a base.

     ```bash
     cp application.example.yml application.local.yml
     ```

     **Example `application.local.yml`:**

     ```yaml
     api_keys:
       - token: "your-secret-token"

     weaviate:
       host: "localhost"
       port: "8001"
       grpc_port: "50051"

     env_vars:
     ```

     - **Create an LLM Config File**

       Create an `llm_config.local.yml` file in the root directory. You can use the provided `llm_config.example.yml` as
       a base.

       ```bash
       cp llm_config.example.yml llm_config.local.yml
       ```

       **Example OpenAI Configuration:**

       ```yaml
       - id: "oai-gpt-41-mini"
         name: "GPT 4.1 Mini"
         description: "GPT 4.1 Mini on OpenAI"
         type: "openai_chat"
         model: "gpt-4.1-mini"
         api_key: "<your_openai_api_key>"
         tools: []
         cost_per_million_input_token: 0.4
         cost_per_million_output_token: 1.6
       ```

       **Example Azure OpenAI Configuration:**

       ```yaml
       - id: "azure-gpt-4-omni"
         name: "GPT 4 Omni"
         description: "GPT 4 Omni on Azure"
         type: "azure_chat"
         endpoint: "<your_azure_model_endpoint>"
         api_version: "2024-02-15-preview"
         azure_deployment: "gpt4o"
         model: "gpt4o"
         api_key: "<your_azure_api_key>"
         tools: []
         cost_per_million_input_token: 0.4
         cost_per_million_output_token: 1.6
       ```

       **Explanation of Configuration Parameters**

       The configuration parameters are used by Pyris's capability system to select the appropriate model for a task.

       **Parameter Descriptions:**
       - `api_key`: The API key for the model.
       - `description`: Additional information about the model.
       - `id`: Unique identifier for the model across all models.
       - `model`: The official name of the model as used by the vendor (e.g., "gpt-4.1", "gpt-4.1-mini").
       - `name`: A custom, human-readable name for the model.
       - `type`: The model type, used to select the appropriate client (e.g., `openai_chat`, `azure_chat`, `ollama`).
       - `endpoint`: The URL to connect to the model.
       - `api_version`: The API version to use with the model.
       - `azure_deployment`: The deployment name of the model on Azure.
       - `tools`: The tools supported by the model.
       - `cost_per_million_input_token`: The cost per million input tokens for the model.
       - `cost_per_million_output_token`: The cost per million output tokens for the model.

     > **Note:** Most existing pipelines in Pyris currently require the full GPT-4.1 model family to be configured.
     > Some features require special models. Watch the logs for any warnings regarding missing models.

6. **Run the Server**

   Start the Pyris server:

   ```bash
   APPLICATION_YML_PATH=./application.local.yml \
   LLM_CONFIG_PATH=./llm_config.local.yml \
   uvicorn app.main:app --reload
   ```

7. **Access API Documentation**

   Open your browser and navigate to [http://localhost:8000/docs](http://localhost:8000/docs) to access the interactive
   API documentation.

#### Testing

To run the tests, run the following command in the `iris` folder:

```bash
poetry run pytest
```

To run the tests with coverage:

```bash
poetry run coverage run -m pytest # Run the tests
poetry run coverage html # Generate the coverage report in HTML
```

#### Linting

To lint the code, you can use pre-commit. Run the following command **in the edutelligence repository root`**:

```bash
pre-commit run --all-files
```

This will run all the pre-commit hooks on all files in the repository.

#### Detect Secrets

To manually scan all Iris files in the repository for secrets, run the following command in the `iris` folder:

```bash
git ls-files -z | xargs -0 detect-secrets-hook --baseline .secrets.baseline
```

To add all current secrets to the `.secrets.baseline` file, run the following command in the `iris` folder:

```bash
git ls-files -z | xargs -0 detect-secrets scan --baseline .secrets.baseline
```

---

### Docker Setup

Deploying Pyris using Docker ensures a consistent environment and simplifies the deployment process.

#### Prerequisites

- **Docker**: Install Docker from the [official website](https://www.docker.com/get-started).
- **Docker Compose**: Comes bundled with Docker Desktop or install separately on Linux.
- **Clone the Pyris Repository**: If not already done, clone the repository.
- **Create Configuration Files**: Create the `application.local.yml` and `llm_config.local.yml` files as described in
  the [Local Development Setup](#local-development-setup) section.

  ```bash
  git clone https://github.com/ls1intum/Pyris.git Pyris
  cd Pyris
  ```

#### Docker Compose Files

- **Development**: `docker-compose/pyris-dev.yml`
- **Production with Nginx**: `docker-compose/pyris-production.yml`
- **Production without Nginx**: `docker-compose/pyris-production-internal.yml`

#### Running the Containers

##### **Development Environment**

1. **Start the Containers**

   ```bash
   docker-compose -f docker-compose/pyris-dev.yml up --build
   ```

   - Builds the Pyris application.
   - Starts Pyris and Weaviate in development mode.
   - Mounts local configuration files for easy modification.

2. **Access the Application**
   - Application URL: [http://localhost:8000](http://localhost:8000)
   - API Docs: [http://localhost:8000/docs](http://localhost:8000/docs)

##### **Production Environment**

###### **Option 1: With Nginx**

1. **Prepare SSL Certificates**
   - Place your SSL certificate (`fullchain.pem`) and private key (`priv_key.pem`) in the specified paths or update the
     paths in the Docker Compose file.

2. **Start the Containers**

   ```bash
   docker-compose -f docker-compose/pyris-production.yml up -d
   ```

   - Pulls the latest Pyris image.
   - Starts Pyris, Weaviate, and Nginx.
   - Nginx handles SSL termination and reverse proxying.

3. **Access the Application**
   - Application URL: `https://your-domain.com`

###### **Option 2: Without Nginx**

1. **Start the Containers**

   ```bash
   docker-compose -f docker-compose/pyris-production-internal.yml up -d
   ```

   - Pulls the latest Pyris image.
   - Starts Pyris and Weaviate.

2. **Access the Application**
   - Application URL: [http://localhost:8000](http://localhost:8000)

---

#### Managing the Containers

- **Stop the Containers**

  ```bash
  docker-compose -f <compose-file> down
  ```

  Replace `<compose-file>` with the appropriate Docker Compose file.

- **View Logs**

  ```bash
  docker-compose -f <compose-file> logs -f <service-name>
  ```

  Example:

  ```bash
  docker-compose -f docker-compose/pyris-dev.yml logs -f pyris-app
  ```

- **Rebuild Containers**

  If you've made changes to the code or configurations:

  ```bash
  docker-compose -f <compose-file> up --build
  ```

#### Customizing Configuration

- **Environment Variables**

  You can customize settings using environment variables:
  - `PYRIS_DOCKER_TAG`: Specifies the Pyris Docker image tag.
  - `PYRIS_APPLICATION_YML_FILE`: Path to your `application.yml` file.
  - `PYRIS_LLM_CONFIG_YML_FILE`: Path to your `llm_config.yml` file.
  - `PYRIS_PORT`: Host port for Pyris application (default is `8000`).
  - `WEAVIATE_PORT`: Host port for Weaviate REST API (default is `8001`).
  - `WEAVIATE_GRPC_PORT`: Host port for Weaviate gRPC interface (default is `50051`).

- **Configuration Files**

  Modify configuration files as needed:
  - **Pyris Configuration**: Update `application.yml` and `llm_config.yml`.
    - `application.yml` must include an `llm_configuration` section that maps pipeline `implementation_id`s to variant IDs and roles with `local`/`cloud` model names. Use `application.example.yml` as a template.
    - Bulk-replace a model name across all `llm_configuration` entries:

      ```bash
      # adjust these three values
      APP_YML=application.yml
      OLD_MODEL=gpt-4.1-mini
      NEW_MODEL=gpt-4o-mini

      python - <<'PY'
      import os
      import pathlib
      import yaml

      path = pathlib.Path(os.environ["APP_YML"])
      old = os.environ["OLD_MODEL"]
      new = os.environ["NEW_MODEL"]

      data = yaml.safe_load(path.read_text(encoding="utf-8"))
      llm_cfg = data.get("llm_configuration", {})

      def walk(node):
          if isinstance(node, dict):
              return {k: walk(v) for k, v in node.items()}
          if isinstance(node, list):
              return [walk(v) for v in node]
          if isinstance(node, str) and node == old:
              return new
          return node

      data["llm_configuration"] = walk(llm_cfg)
      path.write_text(
          yaml.safe_dump(data, sort_keys=False, allow_unicode=True),
          encoding="utf-8",
      )
      print(f"Replaced {old} -> {new} in {path}")
      PY
      ```

  - **Weaviate Configuration**: Adjust settings in `weaviate.yml`.
  - **Nginx Configuration**: Modify Nginx settings in `nginx.yml` and related config files.

## Troubleshooting

- **Port Conflicts**

  If you encounter port conflicts, change the host ports using environment variables:

  ```bash
  export PYRIS_PORT=8080
  ```

- **Permission Issues**

  Ensure you have the necessary permissions for files and directories, especially for SSL certificates.

- **Docker Resources**

  If services fail to start, ensure Docker has sufficient resources allocated.
