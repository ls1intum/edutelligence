{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Expert Evaluation\n",
    "Conduct the expert evaluation by importing the json files from `data/3_submissions_with_categorized_feedback_jsons` into a new expert evaluation in the playground. Download the results of the completed expert evaluation as well as the evaluation configuration.\n",
    "Save the downloaded files in the `data/4_expert_evaluation/input_pseudonymized` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### (Optional) Splitting the Evaluation Config\n",
    "If you want to do multiple, potentially incomplete evaluations, you can split the evaluation config into multiple configs. All configs will contain the same exercises, submissions, feedback suggestions, and metrics. However, the configs will have different exercise and submission orders.\n",
    "Specifically, the configs will have a rotating order of exercises and either an ascending or descending order of submissions.\n",
    "\n",
    "This allows different experts to evaluate the same exercises and submissions, but in a different order. This can help reduce bias in the evaluation process. Additionally, it enables the experts to hand in a incomplete evaluation early, therefore reducing the invested time per expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import copy\n",
    "\n",
    "evaluation_config_path = \"data/4_expert_evaluation/evaluation_config.json\"\n",
    "evaluation_config_splitted_path = \"data/4_expert_evaluation/evaluation_configs_splitted\"\n",
    "\n",
    "os.makedirs(evaluation_config_splitted_path, exist_ok=True)\n",
    "\n",
    "evaluation_config = {}\n",
    "\n",
    "with open(evaluation_config_path, \"r\") as file:\n",
    "    evaluation_config = json.load(file)\n",
    "\n",
    "evaluation_config[\"started\"] = False\n",
    "evaluation_config[\"expertIds\"] = []\n",
    "evaluation_config[\"mappings\"] = {}\n",
    "\n",
    "exercises = evaluation_config[\"exercises\"]\n",
    "\n",
    "\n",
    "def generate_config(starting_index: int, ascending: bool) -> None:\n",
    "    new_config = copy.deepcopy(evaluation_config)\n",
    "    new_config[\"name\"] = (\n",
    "        f\"{evaluation_config['name']}_{'ascending' if ascending else 'descending'}_{starting_index}\"\n",
    "    )\n",
    "    new_config[\"exercises\"] = exercises[starting_index:] + exercises[:starting_index]\n",
    "\n",
    "    for exercise in new_config[\"exercises\"]:\n",
    "        exercise[\"submissions\"].sort(key=lambda x: x[\"id\"], reverse=not ascending)\n",
    "\n",
    "    new_config_path = f\"{evaluation_config_splitted_path}/{new_config['name']}.json\"\n",
    "    with open(new_config_path, \"w\") as new_file:\n",
    "        json.dump(new_config, new_file, indent=4)\n",
    "\n",
    "\n",
    "for ascending in [True, False]:\n",
    "    for starting_index in range(len(exercises)):\n",
    "        generate_config(starting_index, ascending)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Find Evaluation Files\n",
    "Once you have saved the expert evaluation results and the config files in the `data/4_expert_evaluation/input_pseudonymized` directory, run the following cell to create lists of all config and progress files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.json_service import get_evaluation_files\n",
    "\n",
    "data_dir = \"data/4_expert_evaluation/input_pseudonymized\"\n",
    "output_dir = \"data/4_expert_evaluation/output_depseudonymized\"\n",
    "\n",
    "evaluation_config_files, evaluation_progress_files = get_evaluation_files(data_dir)\n",
    "\n",
    "print(f\"Found {len(evaluation_config_files)} evaluation config files.\")\n",
    "print(f\"Found {len(evaluation_progress_files)} evaluation progress files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Create a Common Config File\n",
    "Given multiple config files in the `data/4_expert_evaluation/input_pseudonymized` directory, creates a common config applicable to all progress files and saves it to the `data/4_expert_evaluation/output_depseudonymized` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from service.json_service import create_common_config\n",
    "\n",
    "common_evaluation_config = create_common_config(evaluation_config_files)\n",
    "\n",
    "common_config_path = f\"{output_dir}/common_evaluation_config.json\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(common_config_path, \"w\") as common_file:\n",
    "    json.dump(common_evaluation_config, common_file, indent=4)\n",
    "\n",
    "print(f\"Created Common Evaluation Config With:\")\n",
    "print(f\"- {len(common_evaluation_config.get(\"exercises\", []))} Exercises\")\n",
    "print(f\"- {sum([len(e.get(\"submissions\", [])) for e in common_evaluation_config.get(\"exercises\", [])])} Submissions\")\n",
    "print(f\"- {len(common_evaluation_config.get(\"metrics\", []))} Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Resolve Feedback Types and Metric Titles in Progress Files\n",
    "Resolves the feedback types and Metric Titles for the progress files in the `data/4_expert_evaluation/input_pseudonymized` directory and saves the de-pseudonymized progress files in the `data/4_expert_evaluation/output_depseudonymized` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from service.json_service import resolve_feedback_types_and_metric_titles\n",
    "\n",
    "metric_ids_to_titles = {metric[\"id\"]: metric[\"title\"] for metric in common_evaluation_config.get(\"metrics\", [])}\n",
    "pseudonym_to_feedback_type = common_evaluation_config.get(\"mappings\", {})\n",
    "\n",
    "for progress_file, progress_data in resolve_feedback_types_and_metric_titles(evaluation_progress_files, metric_ids_to_titles, pseudonym_to_feedback_type).items():\n",
    "    output_progress_path = f\"{output_dir}/{os.path.basename(progress_file)}\"\n",
    "    with open(output_progress_path, \"w\") as output_file:\n",
    "        json.dump(progress_data, output_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
