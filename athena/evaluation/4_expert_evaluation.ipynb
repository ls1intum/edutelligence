{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Expert Evaluation\n",
    "Conduct the expert evaluation by importing the json files from `data/3_submissions_with_categorized_feedback_jsons` into a new expert evaluation in the playground. Download the results of the completed expert evaluation as well as the evaluation configuration.\n",
    "Save the downloaded files in the `data/4_expert_evaluation` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Read Expert Evaluation\n",
    "Once you have saved the expert evaluation results and the config file in the `data/4_expert_evaluation` directory, run the following cell to read the expert evaluation results into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.json_service import read_expert_evaluation\n",
    "\n",
    "data_dir = \"data/4_expert_evaluation\"\n",
    "\n",
    "expert_evaluations = read_expert_evaluation(data_dir)\n",
    "expert_evaluations[\"evaluation_type\"] = \"expert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Save the Expert Evaluation\n",
    "Save the expert evaluation results to a CSV file for further analysis and processing in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_evaluations.to_csv(\"data/4_expert_evaluation/expert_evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### (Optional) Splitting the Evaluation Config\n",
    "If you want to do multiple, potentially incomplete evaluations, you can split the evaluation config into multiple configs. All configs will contain the same exercises, submissions, feedback suggestions, and metrics. However, the configs will have different exercise and submission orders.\n",
    "Specifically, the configs will have a rotating order of exercises and either an ascending or descending order of submissions.\n",
    "\n",
    "This allows different experts to evaluate the same exercises and submissions, but in a different order. This can help reduce bias in the evaluation process. Additionally, it enables the experts to hand in a incomplete evaluation early, therefore reducing the invested time per expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import copy\n",
    "\n",
    "evaluation_config_path = \"data/4_expert_evaluation/evaluation_config.json\"\n",
    "evaluation_config_splitted_path = \"data/4_expert_evaluation/evaluation_configs_splitted\"\n",
    "\n",
    "os.makedirs(evaluation_config_splitted_path, exist_ok=True)\n",
    "\n",
    "evaluation_config = {}\n",
    "\n",
    "with open(evaluation_config_path, \"r\") as file:\n",
    "    evaluation_config = json.load(file)\n",
    "\n",
    "evaluation_config[\"started\"] = False\n",
    "evaluation_config[\"expertIds\"] = []\n",
    "evaluation_config[\"mappings\"] = {}\n",
    "\n",
    "exercises = evaluation_config[\"exercises\"]\n",
    "\n",
    "\n",
    "def generate_config(starting_index: int, ascending: bool) -> None:\n",
    "    new_config = copy.deepcopy(evaluation_config)\n",
    "    new_config[\"name\"] = (\n",
    "        f\"{evaluation_config['name']}_{'ascending' if ascending else 'descending'}_{starting_index}\"\n",
    "    )\n",
    "    new_config[\"exercises\"] = exercises[starting_index:] + exercises[:starting_index]\n",
    "\n",
    "    for exercise in new_config[\"exercises\"]:\n",
    "        exercise[\"submissions\"].sort(key=lambda x: x[\"id\"], reverse=not ascending)\n",
    "\n",
    "    new_config_path = f\"{evaluation_config_splitted_path}/{new_config['name']}.json\"\n",
    "    with open(new_config_path, \"w\") as new_file:\n",
    "        json.dump(new_config, new_file, indent=4)\n",
    "\n",
    "\n",
    "for ascending in [True, False]:\n",
    "    for starting_index in range(len(exercises)):\n",
    "        generate_config(starting_index, ascending)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "evaluation-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
