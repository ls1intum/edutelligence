{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.json_service import load_evaluation_progress, load_common_evaluation_config\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "evaluation_progress_path = \"data/4_expert_evaluation/output_depseudonymized/\"\n",
    "llm_evaluation_progress_path = \"data/5_llm_evaluation/evaluation_progress_llm-as-a-judge.json\"\n",
    "\n",
    "evaluation_config_path = \"data/4_expert_evaluation/output_depseudonymized/common_evaluation_config.json\"\n",
    "\n",
    "participant_info_path = \"data/6_analysis/participant_info.csv\"\n",
    "\n",
    "analysis_output_path = \"data/6_analysis/\"\n",
    "\n",
    "# Load evaluation progress\n",
    "df = load_evaluation_progress(evaluation_progress_path, llm_evaluation_progress_path)\n",
    "\n",
    "# Load evaluation config\n",
    "evaluation_config_df = load_common_evaluation_config(evaluation_config_path)\n",
    "df = df.merge(evaluation_config_df, on=[\"exercise_id\", \"submission_id\", \"feedback_type\"], how=\"left\")\n",
    "\n",
    "# Load participant info and add LLM as a participant\n",
    "participant_info_df = pd.read_csv(participant_info_path, delimiter=\";\")\n",
    "participant_info_df = pd.concat([participant_info_df, pd.DataFrame([{\n",
    "    'expert_id': 'llm',\n",
    "    'evaluation_name': 'LLM as a judge',\n",
    "    'link': '',\n",
    "    'name': 'LLM',\n",
    "    'study_program': '',\n",
    "    'semester': pd.NA,\n",
    "    'eist_participation': pd.NA,\n",
    "    'pse_participation': pd.NA,\n",
    "    'tutoring_experience': pd.NA,\n",
    "    'group': 'LLM',\n",
    "}])])\n",
    "df = df.merge(participant_info_df, on=[\"expert_id\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['expert_id', 'study_program', 'semester', 'eist_participation', 'pse_participation', 'tutoring_experience', 'group', 'exercise_id', 'submission_id', 'feedback_type', 'metric', 'score', 'exercise', 'submission', 'feedback']]\n",
    "\n",
    "df.to_csv(os.path.join(analysis_output_path, \"evaluation_data.csv\"), index=False)\n",
    "df = pd.read_csv(os.path.join(analysis_output_path, \"evaluation_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_expert_ids = [\n",
    "    'e9eec3a2-9fe6-4974-a346-45fe43ab0590',  # Low variance and random ratings\n",
    "]\n",
    "\n",
    "df = df[~df['expert_id'].isin(excluded_expert_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set global plot style\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=['#105391', '#68A0C6', '#9BC6E8', '#999999', '#E07430', '#A1AC23', '#DAD7CC'])\n",
    "mpl.rcParams['font.family'] = 'Helvetica Neue'\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # Median Style (Thick Black Line)\n",
    "    'boxplot.medianprops.color': 'black',\n",
    "    'boxplot.medianprops.linewidth': 2.5,\n",
    "    \n",
    "    # Mean Style (White Triangle with Black Border)\n",
    "    'boxplot.meanprops.marker': '^',\n",
    "    'boxplot.meanprops.markerfacecolor': 'white',\n",
    "    'boxplot.meanprops.markeredgecolor': 'black',\n",
    "\n",
    "    # Legend Position (Lower Right)\n",
    "    'legend.loc': 'lower right',\n",
    "\n",
    "    # Figure Size\n",
    "    'figure.figsize': (6, 4),\n",
    "})\n",
    "\n",
    "metric_order = sorted(df['metric'].dropna().unique())\n",
    "feedback_type_order = ['Cofee', 'Tutor', 'LLM']\n",
    "group_order = ['Student', 'Instructor']\n",
    "\n",
    "# # Filter 0 scores\n",
    "# df = df[df['score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for group in [['Student'], ['Instructor'], ['LLM'], ['Student', 'Instructor']]:\n",
    "    df_group = df[df['group'].isin(group)]\n",
    "    plot_boxplot(\n",
    "        df = df_group,\n",
    "        x = 'feedback_type',\n",
    "        y = 'score',\n",
    "        hue = 'metric',\n",
    "        x_order = feedback_type_order,\n",
    "        hue_order = metric_order,\n",
    "        title = f'Assessment Scores by Feedback Type and Metric ({\"s & \".join(group)}s):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Metric',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{\"_\".join(group)}_feedback_type_metric_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for group in [['Student'], ['Instructor'], ['LLM'], ['Student', 'Instructor']]:\n",
    "    df_group = df[df['group'].isin(group)]\n",
    "    plot_boxplot(\n",
    "        df = df_group,\n",
    "        x = 'metric',\n",
    "        y = 'score',\n",
    "        hue = 'feedback_type',\n",
    "        x_order = metric_order,\n",
    "        hue_order = feedback_type_order,\n",
    "        title = f'Assessment Scores by Metric & Feedback Type ({\"s & \".join(group)}s):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Feedback Type',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{\"_\".join(group)}_metric_feedback_type_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for feedback_type in feedback_type_order:\n",
    "    df_feedback = df[df['feedback_type'] == feedback_type]\n",
    "    df_feedback = df_feedback[df_feedback['group'] != 'LLM']\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plot_boxplot(\n",
    "        df = df_feedback,\n",
    "        x = 'metric',\n",
    "        y = 'score',\n",
    "        hue = 'group',\n",
    "        x_order = metric_order,\n",
    "        hue_order = group_order,\n",
    "        title = f'Assessment Scores by Metric & Feedback Source ({feedback_type}):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Group',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{\"_\".join(group)}_metric_group_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for feedback_type in feedback_type_order:\n",
    "    df_feedback = df[df['feedback_type'] == feedback_type]\n",
    "    df_feedback = df_feedback[df_feedback['group'] != 'LLM']\n",
    "    plot_boxplot(\n",
    "        df = df_feedback,\n",
    "        x = 'group',\n",
    "        y = 'score',\n",
    "        hue = 'metric',\n",
    "        x_order = group_order,\n",
    "        hue_order = metric_order,\n",
    "        title = f'Assessment Scores by Feedback Source & Metric ({feedback_type}):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Group',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{feedback_type}_feedback_source_metric_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP & PRE-PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df = evaluation_df.copy()\n",
    "df = df[df['group'] == 'Student']  # Focus on student raters only\n",
    "\n",
    "\n",
    "# Ensure score is numeric\n",
    "# df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "# df['score'] = df['score'].replace(0, np.nan)\n",
    "df = df.dropna(subset=['score'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CALCULATE CONSENSUS & DEVIATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Define what constitutes a \"unique item\" that experts are rating\n",
    "# In your real data, this is likely: ['submission_id', 'feedback_type', 'metric']\n",
    "# If you used the mock data, it's just 'unique_item_id'\n",
    "if 'unique_item_id' in df.columns:\n",
    "    group_cols = ['unique_item_id']\n",
    "else:\n",
    "    group_cols = ['submission_id', 'feedback_type', 'metric']\n",
    "\n",
    "# A. Calculate the Median score for every item (The Peer Consensus)\n",
    "# transform returns a Series aligned with the original dataframe\n",
    "df['consensus_score'] = df.groupby(group_cols)['score'].transform('median')\n",
    "\n",
    "# B. Calculate Absolute Deviation (How far was THIS rating from the median?)\n",
    "df['deviation'] = abs(df['score'] - df['consensus_score'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. AGGREGATE PER EXPERT\n",
    "# ---------------------------------------------------------\n",
    "expert_stats = df.groupby('name').agg(\n",
    "    avg_deviation=('deviation', 'mean'),  # Agreement Score (Lower is better)\n",
    "    std_score=('score', 'std'),           # Variance (Are they using the full scale?)\n",
    "    count=('score', 'count')              # Did they rate enough items?\n",
    ").reset_index()\n",
    "\n",
    "# Filter out experts with too few ratings to be statistically valid\n",
    "expert_stats = expert_stats[expert_stats['count'] > 5]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VISUALIZE\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(\n",
    "    data=expert_stats,\n",
    "    x='avg_deviation',\n",
    "    y='std_score',\n",
    "    size='count',\n",
    "    sizes=(50, 400),\n",
    "    alpha=0.7,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add Threshold Lines (Arbitrary, based on distribution)\n",
    "mean_dev_threshold = expert_stats['avg_deviation'].quantile(0.90) # Top 10% disagree-ers\n",
    "plt.axvline(mean_dev_threshold, color='red', linestyle='--', alpha=0.5, label='High Disagreement Zone')\n",
    "\n",
    "# Label the outliers\n",
    "for i, row in expert_stats.iterrows():\n",
    "    # Label if deviation is high OR variance is suspiciously low\n",
    "    if row['avg_deviation'] > mean_dev_threshold or row['std_score'] < 0.5:\n",
    "        plt.text(\n",
    "            row['avg_deviation']+0.02, \n",
    "            row['std_score'], \n",
    "            row['name'], \n",
    "            fontsize=9, \n",
    "            color='darkred',\n",
    "            weight='bold'\n",
    "        )\n",
    "\n",
    "plt.title('Rater Reliability Analysis: Outlier Detection', fontsize=15)\n",
    "plt.xlabel('Average Disagreement with Peers (Mean Absolute Deviation)\\n(Right = High Disagreement)', fontsize=11)\n",
    "plt.ylabel('Rating Variance (Standard Deviation)\\n(Bottom = Always clicks same button)', fontsize=11)\n",
    "plt.legend(title='Number of Ratings', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. PRINT SUSPICIOUS ID LIST\n",
    "# ---------------------------------------------------------\n",
    "print(\"--- SUSPICIOUS RATERS ---\")\n",
    "# 1. The Disagree-ers (High Deviation)\n",
    "outliers = expert_stats[expert_stats['avg_deviation'] > mean_dev_threshold]\n",
    "print(f\"\\n[High Disagreement] (Consistently deviated from group consensus):\")\n",
    "print(outliers[['name', 'avg_deviation', 'count']].sort_values('avg_deviation', ascending=False))\n",
    "\n",
    "# 2. The Flatliners (Low Variance)\n",
    "flatliners = expert_stats[expert_stats['std_score'] < 0.6]\n",
    "print(f\"\\n[Flatliners] (Variance < 0.6, likely clicked same button repeatedly):\")\n",
    "print(flatliners[['name', 'std_score', 'count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP & PRE-PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df = evaluation_df.copy()\n",
    "df = df[df['group'] == 'Student']  # Focus on student raters only\n",
    "\n",
    "\n",
    "\n",
    "# Ensure score is numeric\n",
    "# df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "# df['score'] = df['score'].replace(0, np.nan)\n",
    "df = df.dropna(subset=['score'])\n",
    "\n",
    "# Safety check for 'name' column if using real data that might be missing it\n",
    "if 'name' not in df.columns:\n",
    "    df['name'] = df['expert_id']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CALCULATE CONSENSUS & DEVIATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Define what constitutes a \"unique item\" that experts are rating\n",
    "if 'unique_item_id' in df.columns:\n",
    "    group_cols = ['unique_item_id']\n",
    "else:\n",
    "    # In real data, an item is defined by the specific submission + feedback + metric\n",
    "    group_cols = ['submission_id', 'feedback_type', 'metric']\n",
    "\n",
    "# A. Calculate the Median score for every item (The Peer Consensus)\n",
    "df['consensus_score'] = df.groupby(group_cols)['score'].transform('median')\n",
    "\n",
    "# B. Calculate Absolute Deviation (How far was THIS rating from the median?)\n",
    "df['deviation'] = abs(df['score'] - df['consensus_score'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. AGGREGATE PER EXPERT\n",
    "# ---------------------------------------------------------\n",
    "# We include 'name' in the aggregation to keep it available\n",
    "expert_stats = df.groupby(['expert_id', 'name']).agg(\n",
    "    avg_deviation=('deviation', 'mean'),  # Agreement Score (Lower is better)\n",
    "    std_score=('score', 'std'),           # Variance (Are they using the full scale?)\n",
    "    count=('score', 'count')              # Did they rate enough items?\n",
    ").reset_index()\n",
    "\n",
    "# Filter out experts with too few ratings to be statistically valid\n",
    "expert_stats = expert_stats[expert_stats['count'] > 5]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VISUALIZE: GLOBAL OUTLIER DETECTION\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(\n",
    "    data=expert_stats,\n",
    "    x='avg_deviation',\n",
    "    y='std_score',\n",
    "    size='count',\n",
    "    sizes=(50, 400),\n",
    "    alpha=0.7,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add Threshold Lines\n",
    "mean_dev_threshold = expert_stats['avg_deviation'].quantile(0.50) # Top 50% disagree-ers\n",
    "plt.axvline(mean_dev_threshold, color='red', linestyle='--', alpha=0.5, label='High Disagreement Zone')\n",
    "\n",
    "# Label the outliers\n",
    "for i, row in expert_stats.iterrows():\n",
    "    # Changed threshold from 0.5 to 0.8\n",
    "    if row['avg_deviation'] > mean_dev_threshold or row['std_score'] < 0.8:\n",
    "        plt.text(\n",
    "            row['avg_deviation']+0.02, \n",
    "            row['std_score'], \n",
    "            row['name'], # Changed to use name\n",
    "            fontsize=9, \n",
    "            color='darkred',\n",
    "            weight='bold'\n",
    "        )\n",
    "\n",
    "plt.title('Rater Reliability Analysis: Outlier Detection', fontsize=15)\n",
    "plt.xlabel('Average Disagreement with Peers (Mean Absolute Deviation)', fontsize=11)\n",
    "plt.ylabel('Rating Variance (Standard Deviation)', fontsize=11)\n",
    "plt.legend(title='Number of Ratings', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. DETAILED ANALYSIS: INDIVIDUAL PLOTS FOR OUTLIERS\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- GENERATING INDIVIDUAL BIAS PLOTS FOR OUTLIERS ---\")\n",
    "\n",
    "# Identify suspicious raters (High Disagreement OR Low Variance)\n",
    "# Changed threshold from 0.5 to 0.8\n",
    "suspicious_mask = (expert_stats['avg_deviation'] > mean_dev_threshold) | (expert_stats['std_score'] < 0.8)\n",
    "suspicious_raters = expert_stats[suspicious_mask]\n",
    "\n",
    "if suspicious_raters.empty:\n",
    "    print(\"No significant outliers found to analyze.\")\n",
    "else:\n",
    "    for idx, rater in suspicious_raters.iterrows():\n",
    "        rater_id = rater['expert_id']\n",
    "        rater_name = rater['name']\n",
    "        \n",
    "        # Get all ratings for this specific person\n",
    "        rater_data = df[df['expert_id'] == rater_id].copy()\n",
    "        \n",
    "        # Add Jitter for visualization (since scores are discrete integers)\n",
    "        # We add random noise (-0.2 to +0.2) so points don't overlap perfectly\n",
    "        rater_data['x_jitter'] = rater_data['consensus_score'] + np.random.uniform(-0.15, 0.15, len(rater_data))\n",
    "        rater_data['y_jitter'] = rater_data['score'] + np.random.uniform(-0.15, 0.15, len(rater_data))\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        \n",
    "        # Plot Rater vs Consensus\n",
    "        plt.scatter(\n",
    "            rater_data['x_jitter'], \n",
    "            rater_data['y_jitter'], \n",
    "            alpha=0.6, \n",
    "            c='teal', \n",
    "            edgecolor='k'\n",
    "        )\n",
    "        \n",
    "        # Perfect Agreement Line (Diagonal)\n",
    "        plt.plot([1, 5], [1, 5], color='red', linestyle='--', linewidth=2, label='Perfect Agreement')\n",
    "        \n",
    "        plt.title(f\"Bias Check: {rater_name}\", fontsize=14)\n",
    "        plt.xlabel(\"Peer Consensus (Median)\", fontsize=12)\n",
    "        plt.ylabel(f\"{rater_name}'s Rating\", fontsize=12)\n",
    "        plt.xlim(0.5, 5.5)\n",
    "        plt.ylim(0.5, 5.5)\n",
    "        plt.grid(True, linestyle='--', alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Interpretation Logic for Title\n",
    "        slope_check = np.mean(rater_data['score'] - rater_data['consensus_score'])\n",
    "        # Changed threshold from 0.5 to 0.8\n",
    "        if abs(slope_check) < 0.3 and rater['std_score'] < 0.5:\n",
    "            interp = \"Likely FLATLINER (Central Tendency)\"\n",
    "        elif slope_check < -0.5:\n",
    "            interp = \"Likely HARSH/CRITICAL Bias\"\n",
    "        elif slope_check > 0.5:\n",
    "            interp = \"Likely LENIENT Bias\"\n",
    "        elif rater['avg_deviation'] > 1.2:\n",
    "            interp = \"Likely RANDOM/NOISY Inputs\"\n",
    "        else:\n",
    "            interp = \"Moderate Disagreement\"\n",
    "            \n",
    "        plt.figtext(0.5, 0.01, f\"Pattern: {interp}\", ha=\"center\", fontsize=10, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. PRINT SUMMARY LIST\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- SUSPICIOUS RATERS SUMMARY ---\")\n",
    "print(suspicious_raters[['name', 'avg_deviation', 'std_score', 'count']].sort_values('avg_deviation', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats # Added for Z-score calculation\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP & PRE-PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df = evaluation_df.copy()\n",
    "df = df[df['group'] == 'Student']  # Focus on student raters only\n",
    "\n",
    "# Ensure score is numeric\n",
    "# df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "# df['score'] = df['score'].replace(0, np.nan)\n",
    "df = df.dropna(subset=['score'])\n",
    "\n",
    "# Safety check for 'name' column if using real data that might be missing it\n",
    "if 'name' not in df.columns:\n",
    "    df['name'] = df['expert_id']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CALCULATE CONSENSUS & DEVIATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Define what constitutes a \"unique item\" that experts are rating\n",
    "if 'unique_item_id' in df.columns:\n",
    "    group_cols = ['unique_item_id']\n",
    "else:\n",
    "    # In real data, an item is defined by the specific submission + feedback + metric\n",
    "    group_cols = ['submission_id', 'feedback_type', 'metric']\n",
    "\n",
    "# A. Calculate the Median score for every item (The Peer Consensus)\n",
    "df['consensus_score'] = df.groupby(group_cols)['score'].transform('median')\n",
    "\n",
    "# B. Calculate Absolute Deviation (How far was THIS rating from the median?)\n",
    "df['deviation'] = abs(df['score'] - df['consensus_score'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. AGGREGATE PER EXPERT\n",
    "# ---------------------------------------------------------\n",
    "# We include 'name' in the aggregation to keep it available\n",
    "expert_stats = df.groupby(['expert_id', 'name']).agg(\n",
    "    avg_deviation=('deviation', 'mean'),  # Agreement Score (Lower is better)\n",
    "    std_score=('score', 'std'),           # Variance (Are they using the full scale?)\n",
    "    count=('score', 'count')              # Did they rate enough items?\n",
    ").reset_index()\n",
    "\n",
    "# Filter out experts with too few ratings to be statistically valid\n",
    "expert_stats = expert_stats[expert_stats['count'] > 5]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. STATISTICAL OUTLIER TEST (Z-SCORES)\n",
    "# ---------------------------------------------------------\n",
    "# Instead of arbitrary thresholds, we look for statistical anomalies.\n",
    "# Z = (Value - Mean) / StdDev. \n",
    "# A Z-score > 1.96 means they are in the top 2.5% of extreme values (p < 0.05).\n",
    "\n",
    "# Test 1: Disagreement Z-Score\n",
    "# Are they significantly more disagreeable than the average student?\n",
    "expert_stats['z_deviation'] = stats.zscore(expert_stats['avg_deviation'])\n",
    "\n",
    "# Test 2: Flatline Z-Score\n",
    "# Is their variance significantly lower than the average student?\n",
    "expert_stats['z_variance'] = stats.zscore(expert_stats['std_score'])\n",
    "\n",
    "# Define Statistical Thresholds (Z > 1.96 corresponds to 95% Confidence Interval)\n",
    "Z_THRESHOLD = 1.96 \n",
    "\n",
    "# Identify Outliers based on Z-Scores\n",
    "# 1. High Disagreement: Z_deviation > 1.96\n",
    "# 2. Flatliner (Low Variance): Z_variance < -1.96 (Significantly LESS variance than others)\n",
    "outlier_mask = (expert_stats['z_deviation'] > Z_THRESHOLD) | (expert_stats['z_variance'] < -Z_THRESHOLD)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. VISUALIZE: GLOBAL OUTLIER DETECTION\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(\n",
    "    data=expert_stats,\n",
    "    x='avg_deviation',\n",
    "    y='std_score',\n",
    "    size='count',\n",
    "    sizes=(50, 400),\n",
    "    alpha=0.7,\n",
    "    edgecolor='black',\n",
    "    hue=outlier_mask, # Color code the statistically significant outliers\n",
    "    palette={True: 'red', False: 'teal'}\n",
    ")\n",
    "\n",
    "# Calculate the raw values that correspond to the Z-Score thresholds for plotting lines\n",
    "mean_dev = expert_stats['avg_deviation'].mean()\n",
    "std_dev = expert_stats['avg_deviation'].std()\n",
    "plot_dev_threshold = mean_dev + (Z_THRESHOLD * std_dev)\n",
    "\n",
    "mean_var = expert_stats['std_score'].mean()\n",
    "std_var = expert_stats['std_score'].std()\n",
    "plot_var_threshold = mean_var - (Z_THRESHOLD * std_var)\n",
    "\n",
    "# Add Threshold Lines\n",
    "plt.axvline(plot_dev_threshold, color='darkred', linestyle='--', alpha=0.5, label=f'High Disagreement (Z > {Z_THRESHOLD})')\n",
    "plt.axhline(plot_var_threshold, color='orange', linestyle='--', alpha=0.5, label=f'Suspected Flatliner (Z < -{Z_THRESHOLD})')\n",
    "\n",
    "# Label the outliers\n",
    "for i, row in expert_stats[outlier_mask].iterrows():\n",
    "    plt.text(\n",
    "        row['avg_deviation']+0.02, \n",
    "        row['std_score'], \n",
    "        f\"{row['name']}\", \n",
    "        fontsize=9, \n",
    "        color='darkred',\n",
    "        weight='bold'\n",
    "    )\n",
    "\n",
    "plt.title('Rater Reliability: Statistical Outlier Detection (Z-Score Method)', fontsize=15)\n",
    "plt.xlabel('Average Disagreement with Peers (Mean Absolute Deviation)', fontsize=11)\n",
    "plt.ylabel('Rating Variance (Standard Deviation)', fontsize=11)\n",
    "plt.legend(title='Statistical Outlier', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. DETAILED ANALYSIS: INDIVIDUAL PLOTS FOR OUTLIERS\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- GENERATING INDIVIDUAL BIAS PLOTS FOR STATISTICAL OUTLIERS ---\")\n",
    "\n",
    "suspicious_raters = expert_stats[outlier_mask]\n",
    "\n",
    "if suspicious_raters.empty:\n",
    "    print(\"No statistically significant outliers found (p < 0.05). Your data is clean!\")\n",
    "else:\n",
    "    for idx, rater in suspicious_raters.iterrows():\n",
    "        rater_id = rater['expert_id']\n",
    "        rater_name = rater['name']\n",
    "        \n",
    "        # Get all ratings for this specific person\n",
    "        rater_data = df[df['expert_id'] == rater_id].copy()\n",
    "        \n",
    "        # Add Jitter for visualization\n",
    "        rater_data['x_jitter'] = rater_data['consensus_score'] + np.random.uniform(-0.15, 0.15, len(rater_data))\n",
    "        rater_data['y_jitter'] = rater_data['score'] + np.random.uniform(-0.15, 0.15, len(rater_data))\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        \n",
    "        # Plot Rater vs Consensus\n",
    "        plt.scatter(\n",
    "            rater_data['x_jitter'], \n",
    "            rater_data['y_jitter'], \n",
    "            alpha=0.6, \n",
    "            c='red', \n",
    "            edgecolor='k'\n",
    "        )\n",
    "        \n",
    "        # Perfect Agreement Line (Diagonal)\n",
    "        plt.plot([0, 5], [0, 5], color='grey', linestyle='--', linewidth=2, label='Perfect Agreement')\n",
    "        \n",
    "        plt.title(f\"Bias Check: {rater_name} (Z-Dev: {rater['z_deviation']:.2f})\", fontsize=14)\n",
    "        plt.xlabel(\"Peer Consensus (Median)\", fontsize=12)\n",
    "        plt.ylabel(f\"{rater_name}'s Rating\", fontsize=12)\n",
    "        plt.xlim(-0.5, 5.5)\n",
    "        plt.ylim(-0.5, 5.5)\n",
    "        plt.grid(True, linestyle='--', alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Interpretation Logic\n",
    "        interp = \"Unknown Pattern\"\n",
    "        if rater['z_variance'] < -Z_THRESHOLD:\n",
    "            interp = \"FLATLINER (Statistically Low Variance)\"\n",
    "        elif rater['z_deviation'] > Z_THRESHOLD:\n",
    "             # Check slope/bias\n",
    "            slope_check = np.mean(rater_data['score'] - rater_data['consensus_score'])\n",
    "            if slope_check < -0.5: interp = \"High Disagreement (Harsh Bias)\"\n",
    "            elif slope_check > 0.5: interp = \"High Disagreement (Lenient Bias)\"\n",
    "            else: interp = \"High Disagreement (Random/Noisy)\"\n",
    "\n",
    "        plt.figtext(0.5, -0.02, f\"Diagnosis: {interp}\", ha=\"center\", fontsize=10, bbox={\"facecolor\":\"salmon\", \"alpha\":0.2, \"pad\":5})\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. PRINT SUMMARY LIST\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- STATISTICAL OUTLIERS (Z > 1.96 or Z < -1.96) ---\")\n",
    "summary_cols = ['name', 'avg_deviation', 'z_deviation', 'std_score', 'z_variance']\n",
    "print(suspicious_raters[summary_cols].sort_values('z_deviation', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP\n",
    "# ---------------------------------------------------------\n",
    "df = evaluation_df.copy()\n",
    "\n",
    "# df = df[df['Expert'] == True]  # Experts only\n",
    "# df = df[df['Expert'] == False]  # Students only\n",
    "# df = df[df['expert_id'] != \"llm\"] # Exclude LLM judge for this analysis\n",
    "df = df[df['group'] != \"LLM\"] # LLM judge only\n",
    "\n",
    "# Pre-processing\n",
    "df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "df['score'] = df['score'].replace(0, np.nan)\n",
    "\n",
    "unique_metrics = df['metric'].unique()\n",
    "\n",
    "print(f\"Total raw ratings: {len(df)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. ROBUST ANALYSIS LOOP (RANK-THEN-AGGREGATE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "for metric in unique_metrics:\n",
    "    print(f\"\\n\\n================ {metric.upper()} (SUBMISSION-LEVEL / RANK-BASED) ================\")\n",
    "    \n",
    "    # Filter by metric\n",
    "    df_metric = df[df['metric'] == metric]\n",
    "    \n",
    "    # --- STEP 1: PIVOT TO STUDENT LEVEL ---\n",
    "    # We need to see the student's side-by-side comparison first to calculate ranks.\n",
    "    student_level = df_metric.pivot_table(\n",
    "        index=['expert_id', 'submission_id'], # Unique session\n",
    "        columns='feedback_type', \n",
    "        values='score'\n",
    "    )\n",
    "    \n",
    "    # Remove incomplete sessions (Student didn't rate all 3 techniques)\n",
    "    # This is crucial for valid ranking.\n",
    "    student_level = student_level.dropna()\n",
    "    \n",
    "    print(f\"Valid Student Ratings (Raw Count): {len(student_level)}\")\n",
    "    \n",
    "    if len(student_level) < 5:\n",
    "        print(\"Not enough data to proceed.\")\n",
    "        continue\n",
    "\n",
    "    # --- STEP 2: CALCULATE RANKS (Per Student) ---\n",
    "    # This neutralizes the \"Grumpy vs Happy\" grader bias immediately.\n",
    "    # A '5' from a happy grader becomes Rank 3. A '2' from a grumpy grader also becomes Rank 3.\n",
    "    student_ranks = student_level.rank(axis=1, method='average', ascending=True)\n",
    "    \n",
    "    # --- STEP 3: AGGREGATE BY SUBMISSION (Median of Ranks) ---\n",
    "    # Now we combine the ranks for the same submission.\n",
    "    # We use groupby on the 'submission_id' level of the index.\n",
    "    submission_level = student_ranks.groupby(level='submission_id').median()\n",
    "    \n",
    "    # Drop submissions that might have become incomplete during aggregation \n",
    "    # (unlikely here given previous dropna, but good safety)\n",
    "    clean_data = submission_level.dropna()\n",
    "    \n",
    "    print(f\"Valid Submissions for Analysis: {len(clean_data)}\")\n",
    "    \n",
    "    if len(clean_data) < 5:\n",
    "        print(\"Not enough submissions.\")\n",
    "        continue\n",
    "\n",
    "    # --- STEP 4: FRIEDMAN TEST (On Median Ranks) ---\n",
    "    # We are now comparing the \"Consensus Rank\" of T1 vs T2 vs T3\n",
    "    \n",
    "    # Calculate mean of the median ranks for display\n",
    "    final_mean_ranks = clean_data.mean().sort_values(ascending=False)\n",
    "    \n",
    "    stat, p_value = stats.friedmanchisquare(*[clean_data[col] for col in clean_data.columns])\n",
    "    \n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"Friedman p-value: {p_value:.5f}\")\n",
    "    print(\"Mean of Median Ranks (Higher is better):\")\n",
    "    print(final_mean_ranks)\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n--- Post-Hoc: Nemenyi Test ---\")\n",
    "        \n",
    "        nemenyi = sp.posthoc_nemenyi_friedman(clean_data.reset_index(drop=True))\n",
    "        \n",
    "        # Filter for significant pairs to make reading easier\n",
    "        print(\"Significant Differences (p < 0.05):\")\n",
    "        cols = clean_data.columns\n",
    "        any_sig = False\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(i+1, len(cols)):\n",
    "                t1, t2 = cols[i], cols[j]\n",
    "                p_val = nemenyi.loc[t1, t2]\n",
    "                if p_val < 0.05:\n",
    "                    any_sig = True\n",
    "                    # Determine winner based on mean rank\n",
    "                    winner = t1 if final_mean_ranks[t1] > final_mean_ranks[t2] else t2\n",
    "                    loser = t2 if winner == t1 else t1\n",
    "                    print(f\"  * {winner} beats {loser} (p={p_val:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  - No significant difference between {t1} and {t2} (p={p_val:.4f})\")\n",
    "        \n",
    "        if not any_sig:\n",
    "            print(\"  (None found despite global significance)\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Result: No significant difference found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
