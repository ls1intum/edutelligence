{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383721c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Initialize lists to store the hierarchical data\n",
    "labels = []\n",
    "parents = []\n",
    "values = []\n",
    "\n",
    "# Root node\n",
    "labels.append(\"All Exercises\")\n",
    "parents.append(\"\")\n",
    "values.append(0)  # Root node value\n",
    "\n",
    "# Exercises\n",
    "num_exercises = 4\n",
    "submissions_per_exercise = 25\n",
    "feedback_types = [\"Tutor\", \"LLM\", \"CoFee\"]\n",
    "metrics = [\"Completeness\", \"Correctness\", \"Actionability\", \"Tone\"]\n",
    "\n",
    "for ex in range(1, num_exercises + 1):\n",
    "    exercise_label = f\"Exercise {ex}\"\n",
    "    labels.append(exercise_label)\n",
    "    parents.append(\"All Exercises\")\n",
    "    values.append(0)  # Exercise node value\n",
    "\n",
    "    # Submissions (grouped as \"25 submissions\")\n",
    "    submission_label = f\"{exercise_label} - 25 Submissions\"\n",
    "    labels.append(submission_label)\n",
    "    parents.append(exercise_label)\n",
    "    values.append(0)  # Grouped submissions node value\n",
    "\n",
    "    # Feedback Types\n",
    "    for feedback in feedback_types:\n",
    "        feedback_label = f\"{submission_label} - {feedback} Feedback\"\n",
    "        labels.append(feedback_label)\n",
    "        parents.append(submission_label)\n",
    "        values.append(0)  # Feedback node value\n",
    "\n",
    "        # Metrics\n",
    "        for metric in metrics:\n",
    "            metric_label = f\"{feedback_label} - {metric}\"\n",
    "            labels.append(metric_label)\n",
    "            parents.append(feedback_label)\n",
    "            values.append(1)  # Metric node value\n",
    "\n",
    "# Create the icicle chart\n",
    "fig = go.Figure(go.Icicle(\n",
    "    labels=labels,\n",
    "    parents=parents,\n",
    "    values=values,\n",
    "    tiling=dict(orientation='v'),  # 'v' for vertical orientation (root at the top)\n",
    "    marker=dict(colorscale='Blues')  # Modern color scale\n",
    "))\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title='Hierarchical Structure of Exercises, Submissions, Feedback, and Metrics',\n",
    "    margin=dict(t=50, l=25, r=25, b=25),\n",
    "    template='plotly_white'  # Modern-looking template\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490776aca641d",
   "metadata": {},
   "source": [
    "## Examples of Analysing the Sampled Exercises\n",
    "The following examples demonstrate some basic analysis of the sampled exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab6c3fc0ba5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/1_exercises/exercises.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723b13cdd19ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_submissions = data[\"submission_id\"].nunique()\n",
    "print(f\"Overall number of submissions: {overall_submissions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668553e1c82343d2",
   "metadata": {},
   "source": [
    "Creates a grouped DataFrame to count the number of distinct feedback IDs, submissions, and total feedbacks per score.\n",
    "Saves the data to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabeffbab00ad3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = (\n",
    "    data\n",
    "    .groupby([\"exercise_id\", \"result_score\"])\n",
    "    .agg(\n",
    "        distinct_feedback_count=(\"feedback_id\", \"nunique\"),  # Count distinct feedback IDs per score\n",
    "        submission_count=(\"submission_id\", \"nunique\"),       # Count distinct submissions per score\n",
    "        feedback_count=(\"feedback_id\", \"nunique\")            # Total feedbacks per score\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "total_feedbacks_per_exercise = (\n",
    "    data\n",
    "    .groupby(\"exercise_id\")[\"feedback_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"feedback_id\": \"total_feedback_count\"})\n",
    ")\n",
    "\n",
    "total_submissions_per_exercise = (\n",
    "    data\n",
    "    .groupby(\"exercise_id\")[\"submission_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"submission_id\": \"total_submission_count\"})\n",
    ")\n",
    "\n",
    "# Merge the total feedback count and total submission count back into the grouped data\n",
    "grouped_data = grouped_data.merge(total_feedbacks_per_exercise, on=\"exercise_id\")\n",
    "grouped_data = grouped_data.merge(total_submissions_per_exercise, on=\"exercise_id\")\n",
    "\n",
    "# Calculate average number of feedbacks per exercise and score\n",
    "grouped_data[\"avg_feedbacks_per_score\"] = (\n",
    "    grouped_data[\"feedback_count\"] / grouped_data[\"submission_count\"]\n",
    ")\n",
    "\n",
    "grouped_data = grouped_data[[\n",
    "    \"exercise_id\",\n",
    "    \"result_score\",\n",
    "    \"submission_count\",\n",
    "    \"total_submission_count\",\n",
    "    \"total_feedback_count\",\n",
    "    \"feedback_count\",\n",
    "    \"avg_feedbacks_per_score\"\n",
    "]]\n",
    "\n",
    "grouped_data.to_csv(\"../data/6_analysis/grouped_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e5034a2b7af53",
   "metadata": {},
   "source": [
    "Visualize the relationship between the scores and the average number of feedbacks per score using the grouped data from the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acff50e6065513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the grouped data\n",
    "grouped_data = pd.read_csv(\"../data/6_analysis/grouped_data.csv\")\n",
    "\n",
    "# Create a color and marker map for exercises\n",
    "exercise_ids = grouped_data[\"exercise_id\"].unique()\n",
    "colors = plt.cm.tab10(range(len(exercise_ids)))  # Use a colormap for distinct colors\n",
    "markers = ['o', 's', 'D', '^', 'v', 'P', '*', 'X']  # Different marker styles\n",
    "marker_map = {exercise_id: markers[i % len(markers)] for i, exercise_id in enumerate(exercise_ids)}\n",
    "color_map = {exercise_id: colors[i] for i, exercise_id in enumerate(exercise_ids)}\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for exercise_id in exercise_ids:\n",
    "    subset = grouped_data[grouped_data[\"exercise_id\"] == exercise_id]\n",
    "    x = subset[\"avg_feedbacks_per_score\"]\n",
    "    y = subset[\"result_score\"]\n",
    "\n",
    "    # Scatter points\n",
    "    plt.scatter(\n",
    "        x, y,\n",
    "        label=f\"Exercise {exercise_id}\",\n",
    "        color=color_map[exercise_id],\n",
    "        marker=marker_map[exercise_id],\n",
    "        s=100,  # Marker size\n",
    "        alpha=0.7  # Transparency\n",
    "    )\n",
    "\n",
    "    # Compute regression line\n",
    "    if len(subset) > 1:  # Regression is meaningful only if there are multiple points\n",
    "        coefficients = np.polyfit(x, y, 1)  # Linear regression (degree=1)\n",
    "        regression_line = np.poly1d(coefficients)\n",
    "        plt.plot(\n",
    "            x, regression_line(x),\n",
    "            color=color_map[exercise_id],\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Average Number of Feedbacks\", fontsize=12)\n",
    "plt.ylabel(\"Scores\", fontsize=12)\n",
    "plt.title(\"Scores vs. Average Number of Feedbacks\", fontsize=14)\n",
    "plt.legend(title=\"Exercises\", loc=\"upper left\", fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569b3aec0f0d64f",
   "metadata": {},
   "source": [
    "## Example of Analysing the Sampled Submissions with Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f9df64aae0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_submissions_with_feedback = pd.read_csv(\"data/3_feedback_suggestions/feedback_suggestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055fb4b65658bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = (\n",
    "    sampled_submissions_with_feedback\n",
    "    .groupby([\"exercise_id\", \"result_score\"])\n",
    "    .agg(\n",
    "        submission_count=(\"submission_id\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "total_submissions_per_exercise = (\n",
    "    sampled_submissions_with_feedback\n",
    "    .groupby(\"exercise_id\")[\"submission_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"submission_id\": \"total_submission_count\"})\n",
    ")\n",
    "grouped_data = grouped_data.merge(total_submissions_per_exercise, on=\"exercise_id\", how=\"left\")\n",
    "\n",
    "feedback_types = sampled_submissions_with_feedback[\"feedback_type\"].unique()\n",
    "for feedback_type in feedback_types:\n",
    "    feedback_data = sampled_submissions_with_feedback[sampled_submissions_with_feedback[\"feedback_type\"] == feedback_type]\n",
    "\n",
    "    feedback_count = (\n",
    "        feedback_data\n",
    "        .groupby([\"exercise_id\", \"result_score\"])[\"feedback_id\"]\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"feedback_id\": f\"feedback_count_{feedback_type}\"})\n",
    "    )\n",
    "    grouped_data = grouped_data.merge(feedback_count, on=[\"exercise_id\", \"result_score\"], how=\"left\")\n",
    "    grouped_data[f\"feedback_count_{feedback_type}\"] = grouped_data[f\"feedback_count_{feedback_type}\"].fillna(0).astype(int)\n",
    "\n",
    "    total_feedback_count = (\n",
    "        feedback_data\n",
    "        .groupby(\"exercise_id\")[\"feedback_id\"]\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"feedback_id\": f\"total_feedback_count_{feedback_type}\"})\n",
    "    )\n",
    "    grouped_data = grouped_data.merge(total_feedback_count, on=\"exercise_id\", how=\"left\")\n",
    "    grouped_data[f\"total_feedback_count_{feedback_type}\"] = grouped_data[f\"total_feedback_count_{feedback_type}\"].fillna(0).astype(int)\n",
    "\n",
    "    grouped_data[f\"average_feedback_count_{feedback_type}\"] = (\n",
    "        grouped_data[f\"feedback_count_{feedback_type}\"] / grouped_data[\"submission_count\"]\n",
    "    ).fillna(0)\n",
    "\n",
    "\n",
    "grouped_data.to_csv(\"../data/2_feedback_counts.csv\", index=False)\n",
    "grouped_data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
