{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "evaluation_progress_path = \"data/4_expert_evaluation/output_depseudonymized/\"\n",
    "llm_evaluation_progress_path = \"data/5_llm_evaluation/evaluation_progress_llm-as-a-judge.json\"\n",
    "\n",
    "evaluation_config_path = \"data/4_expert_evaluation/output_depseudonymized/common_evaluation_config.json\"\n",
    "\n",
    "participant_info_path = \"data/6_analysis/participant_info.csv\"\n",
    "\n",
    "common_evaluation_config = {}\n",
    "evaluation_progresses = {}\n",
    "\n",
    "# Load common evaluation config\n",
    "with open(evaluation_config_path, \"r\") as config_file:\n",
    "    common_evaluation_config = json.load(config_file)\n",
    "\n",
    "# Load expert evaluation progress files\n",
    "files = os.listdir(evaluation_progress_path)\n",
    "for file in files:\n",
    "    if file.startswith(\"evaluation_progress_\"):\n",
    "        with open(os.path.join(evaluation_progress_path, file), \"r\") as progress_file:\n",
    "            evaluation_progress = json.load(progress_file)\n",
    "            expert_id = file.replace(\"evaluation_progress_\", \"\").replace(\".json\", \"\")\n",
    "            evaluation_progresses[expert_id] = evaluation_progress\n",
    "\n",
    "# Load optional LLM evaluation progress file\n",
    "if os.path.exists(llm_evaluation_progress_path):\n",
    "    with open(llm_evaluation_progress_path, \"r\") as llm_progress_file:\n",
    "        llm_evaluation_progress = json.load(llm_progress_file)\n",
    "        evaluation_progresses[\"llm\"] = llm_evaluation_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataframe from progress data\n",
    "records = []\n",
    "for expert_id, evaluation_progress in evaluation_progresses.items():\n",
    "    for exercise_id, exercise_data in evaluation_progress.get(\"selected_values\", {}).items():\n",
    "        for submission_id, submission_data in exercise_data.items():\n",
    "            for feedback_type, feedback_data in submission_data.items():\n",
    "                for metric, score in feedback_data.items():\n",
    "                    if metric == \"meta\":\n",
    "                        continue\n",
    "                    record = {\n",
    "                        \"expert_id\": expert_id,\n",
    "                        \"exercise_id\": exercise_id,\n",
    "                        \"submission_id\": submission_id,\n",
    "                        \"feedback_type\": feedback_type,\n",
    "                        \"metric\": metric,\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "                    records.append(record)\n",
    "\n",
    "evaluation_df = pd.DataFrame.from_records(records).astype({\n",
    "        'expert_id': 'string',\n",
    "        'exercise_id': 'int64',\n",
    "        'submission_id': 'int64',\n",
    "        'feedback_type': 'string',\n",
    "        'metric': 'string',\n",
    "        'score': 'float64',\n",
    "    })\n",
    "\n",
    "# Dataframe from participant info\n",
    "if os.path.exists(participant_info_path):\n",
    "    participant_info_df = pd.read_csv(participant_info_path, delimiter=\";\").astype({\n",
    "        'expert_id': 'string',\n",
    "        'evaluation_name': 'string',\n",
    "        'link': 'string',\n",
    "        'name': 'string',\n",
    "        'study_program': 'string',\n",
    "        'semester': 'Int64',\n",
    "        'eist_participation': 'boolean',\n",
    "        'pse_participation': 'boolean',\n",
    "        'tutoring_experience': 'boolean',\n",
    "        'group': 'string',\n",
    "    })\n",
    "    participant_info_df = pd.concat([participant_info_df, pd.DataFrame([{\n",
    "        'expert_id': 'llm',\n",
    "        'evaluation_name': 'LLM as a judge',\n",
    "        'link': '',\n",
    "        'name': 'LLM',\n",
    "        'study_program': '',\n",
    "        'semester': pd.NA,\n",
    "        'eist_participation': pd.NA,\n",
    "        'pse_participation': pd.NA,\n",
    "        'tutoring_experience': pd.NA,\n",
    "        'group': 'LLM',\n",
    "    }])])\n",
    "    evaluation_df = evaluation_df.join(participant_info_df.set_index(\"expert_id\"), on=\"expert_id\", how=\"left\")\n",
    "\n",
    "# Dataframe from common evaluation config\n",
    "records = []\n",
    "for raw_exercise in common_evaluation_config.get(\"exercises\", []):\n",
    "    # Get exercise without submissions\n",
    "    exercise = {k: v for k, v in raw_exercise.items() if k != \"submissions\"}\n",
    "    for raw_submission in raw_exercise.get(\"submissions\", []):\n",
    "        submission = {k: v for k, v in raw_submission.items() if k != \"feedbacks\"}\n",
    "        for feedback_type, raw_feedback in raw_submission.get(\"feedbacks\", {}).items():\n",
    "            record = {\n",
    "                'exercise_id': exercise['id'],\n",
    "                'submission_id': submission['id'],\n",
    "                'feedback_type': feedback_type,\n",
    "                'exercise': exercise,\n",
    "                'submission': submission,\n",
    "                'feedback': raw_feedback,\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "exercise_config_df = pd.DataFrame.from_records(records).astype({\n",
    "        'exercise_id': 'int64',\n",
    "        'submission_id': 'int64',\n",
    "        'feedback_type': 'string',\n",
    "    })\n",
    "\n",
    "evaluation_df = evaluation_df.merge(\n",
    "    exercise_config_df,\n",
    "    on=[\"exercise_id\", \"submission_id\", \"feedback_type\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only required columns\n",
    "df = evaluation_df[[\n",
    "    'expert_id',\n",
    "    'study_program',\n",
    "    'semester',\n",
    "    'eist_participation',\n",
    "    'pse_participation',\n",
    "    'tutoring_experience',\n",
    "    'group',\n",
    "    'exercise_id',\n",
    "    'submission_id',\n",
    "    'feedback_type',\n",
    "    'metric',\n",
    "    'score',\n",
    "    'exercise',\n",
    "    'submission',\n",
    "    'feedback',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import scipy.stats as stats\n",
    "# import scikit_posthocs as sp\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 1. SETUP\n",
    "# # ---------------------------------------------------------\n",
    "# # Assuming 'evaluation_df' is already loaded in your Jupyter Notebook environment.\n",
    "# # We create a copy to avoid modifying the original dataframe during processing.\n",
    "# try:\n",
    "#     df = evaluation_df.copy()\n",
    "#     # Experts only where Expert is true\n",
    "#     # df = df[df['Expert'] == True]\n",
    "#     # Students only where Expert is false\n",
    "#     # df = df[df['Expert'] == False]\n",
    "# except NameError:\n",
    "#     print(\"Error: 'evaluation_df' is not defined. Please ensure your dataframe is loaded.\")\n",
    "#     # For testing purposes only, you might uncomment the line below if loading from CSV\n",
    "#     # df = pd.read_csv('your_data.csv')\n",
    "#     df = pd.DataFrame() # Empty placeholder to prevent further errors if run standalone\n",
    "\n",
    "# print(\"--- Data Loaded ---\")\n",
    "# if not df.empty:\n",
    "#     print(f\"Total Rows: {len(df)}\")\n",
    "#     if 'metric' in df.columns:\n",
    "#         print(f\"Metrics found: {df['metric'].unique()}\")\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 2. PRE-PROCESSING\n",
    "# # ---------------------------------------------------------\n",
    "\n",
    "# if not df.empty:\n",
    "#     # Convert 0 to NaN (as 0 means \"Not Ratable\", not \"Terrible\")\n",
    "#     # Ensure 'score' is numeric just in case\n",
    "#     df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "#     df['score'] = df['score'].replace(0, np.nan)\n",
    "\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 3. STATISTICAL ANALYSIS LOOP (Per Metric)\n",
    "#     # ---------------------------------------------------------\n",
    "\n",
    "#     unique_metrics = df['metric'].unique()\n",
    "\n",
    "#     for metric in unique_metrics:\n",
    "#         print(f\"\\n\\n================ ANALYSIS FOR: {metric.upper()} ================\")\n",
    "        \n",
    "#         # 3.1 Filter\n",
    "#         df_metric = df[df['metric'] == metric]\n",
    "        \n",
    "#         # 3.2 Pivot\n",
    "#         pivoted = df_metric.pivot_table(\n",
    "#             index=['expert_id', 'exercise_id', 'submission_id'], \n",
    "#             columns='feedback_type', \n",
    "#             values='score'\n",
    "#         )\n",
    "        \n",
    "#         # 3.3 DROP MISSING (Strict Matched Pairs)\n",
    "#         clean_data = pivoted.dropna()\n",
    "        \n",
    "#         print(f\"Valid comparison tuples (Expert + Submission): {len(clean_data)}\")\n",
    "        \n",
    "#         if len(clean_data) < 5:\n",
    "#             print(\"Not enough data points for this metric.\")\n",
    "#             continue\n",
    "\n",
    "#         # ---------------------------------------------------------\n",
    "#         # 3.4 ASSUMPTION TESTING (The Pre-conditions)\n",
    "#         # ---------------------------------------------------------\n",
    "#         print(\"\\n--- Precondition Check: Normality (Shapiro-Wilk) ---\")\n",
    "#         print(\"Rationale: If p < 0.05, data is NON-NORMAL. This justifies using Friedman instead of ANOVA.\")\n",
    "        \n",
    "#         violated_normality = False\n",
    "#         for tech in clean_data.columns:\n",
    "#             # Shapiro-Wilk test for normality\n",
    "#             # We need at least 3 data points for Shapiro-Wilk\n",
    "#             if len(clean_data[tech]) >= 3:\n",
    "#                 stat, p_shapiro = stats.shapiro(clean_data[tech])\n",
    "                \n",
    "#                 # Helper text for interpretation\n",
    "#                 normality_status = \"Normal\" if p_shapiro > 0.05 else \"Non-Normal (Violated)\"\n",
    "#                 if p_shapiro < 0.05: violated_normality = True\n",
    "                    \n",
    "#                 print(f\"  {tech}: p={p_shapiro:.5f} -> {normality_status}\")\n",
    "#             else:\n",
    "#                 print(f\"  {tech}: Not enough data for Normality test\")\n",
    "        \n",
    "#         if violated_normality:\n",
    "#             print(\"-> CONCLUSION: Normality assumption violated. Friedman Test is the CORRECT choice.\")\n",
    "#         else:\n",
    "#             print(\"-> CONCLUSION: Data looks Normal. You *could* use Repeated Measures ANOVA, but Friedman is still safe.\")\n",
    "\n",
    "\n",
    "#         # ---------------------------------------------------------\n",
    "#         # 3.5 HYPOTHESIS TESTING\n",
    "#         # ---------------------------------------------------------\n",
    "\n",
    "#         # Rank data (handling the ordinal/bias nature)\n",
    "#         ranked_data = clean_data.rank(axis=1, method='average', ascending=True)\n",
    "#         mean_ranks = ranked_data.mean().sort_values(ascending=False)\n",
    "        \n",
    "#         print(\"\\n--- Friedman Test ---\")\n",
    "#         stat, p_value = stats.friedmanchisquare(*[clean_data[col] for col in clean_data.columns])\n",
    "#         print(f\"p-value: {p_value:.5f}\")\n",
    "#         print(\"Mean Ranks (Higher is better):\")\n",
    "#         print(mean_ranks)\n",
    "        \n",
    "#         if p_value < 0.05:\n",
    "#             print(\"\\n--- Post-Hoc: Nemenyi Test ---\")\n",
    "            \n",
    "#             nemenyi = sp.posthoc_nemenyi_friedman(clean_data.reset_index(drop=True))\n",
    "            \n",
    "#             # Filter for significant pairs to make reading easier\n",
    "#             print(\"Significant Differences (p < 0.05):\")\n",
    "#             cols = clean_data.columns\n",
    "#             any_sig = False\n",
    "#             for i in range(len(cols)):\n",
    "#                 for j in range(i+1, len(cols)):\n",
    "#                     t1, t2 = cols[i], cols[j]\n",
    "#                     p_val = nemenyi.loc[t1, t2]\n",
    "#                     if p_val < 0.05:\n",
    "#                         any_sig = True\n",
    "#                         # Determine winner based on mean rank\n",
    "#                         winner = t1 if mean_ranks[t1] > mean_ranks[t2] else t2\n",
    "#                         loser = t2 if winner == t1 else t1\n",
    "#                         print(f\"  * {winner} beats {loser} (p={p_val:.4f})\")\n",
    "            \n",
    "#             if not any_sig:\n",
    "#                 print(\"  (None found despite global significance)\")\n",
    "                \n",
    "#         else:\n",
    "#             print(\"Result: No significant difference found.\")\n",
    "# else:\n",
    "#     print(\"DataFrame is empty. Please check your data source.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP\n",
    "# ---------------------------------------------------------\n",
    "df = evaluation_df.copy()\n",
    "\n",
    "# df = df[df['Expert'] == True]  # Experts only\n",
    "# df = df[df['Expert'] == False]  # Students only\n",
    "# df = df[df['expert_id'] != \"llm\"] # Exclude LLM judge for this analysis\n",
    "df = df[df['expert_id'] == \"llm\"] # LLM judge only\n",
    "\n",
    "# Pre-processing\n",
    "df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "df['score'] = df['score'].replace(0, np.nan)\n",
    "\n",
    "unique_metrics = df['metric'].unique()\n",
    "\n",
    "print(f\"Total raw ratings: {len(df)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. ROBUST ANALYSIS LOOP (RANK-THEN-AGGREGATE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "for metric in unique_metrics:\n",
    "    print(f\"\\n\\n================ {metric.upper()} (SUBMISSION-LEVEL / RANK-BASED) ================\")\n",
    "    \n",
    "    # Filter by metric\n",
    "    df_metric = df[df['metric'] == metric]\n",
    "    \n",
    "    # --- STEP 1: PIVOT TO STUDENT LEVEL ---\n",
    "    # We need to see the student's side-by-side comparison first to calculate ranks.\n",
    "    student_level = df_metric.pivot_table(\n",
    "        index=['expert_id', 'submission_id'], # Unique session\n",
    "        columns='feedback_type', \n",
    "        values='score'\n",
    "    )\n",
    "    \n",
    "    # Remove incomplete sessions (Student didn't rate all 3 techniques)\n",
    "    # This is crucial for valid ranking.\n",
    "    student_level = student_level.dropna()\n",
    "    \n",
    "    print(f\"Valid Student Ratings (Raw Count): {len(student_level)}\")\n",
    "    \n",
    "    if len(student_level) < 5:\n",
    "        print(\"Not enough data to proceed.\")\n",
    "        continue\n",
    "\n",
    "    # --- STEP 2: CALCULATE RANKS (Per Student) ---\n",
    "    # This neutralizes the \"Grumpy vs Happy\" grader bias immediately.\n",
    "    # A '5' from a happy grader becomes Rank 3. A '2' from a grumpy grader also becomes Rank 3.\n",
    "    student_ranks = student_level.rank(axis=1, method='average', ascending=True)\n",
    "    \n",
    "    # --- STEP 3: AGGREGATE BY SUBMISSION (Median of Ranks) ---\n",
    "    # Now we combine the ranks for the same submission.\n",
    "    # We use groupby on the 'submission_id' level of the index.\n",
    "    submission_level = student_ranks.groupby(level='submission_id').median()\n",
    "    \n",
    "    # Drop submissions that might have become incomplete during aggregation \n",
    "    # (unlikely here given previous dropna, but good safety)\n",
    "    clean_data = submission_level.dropna()\n",
    "    \n",
    "    print(f\"Valid Submissions for Analysis: {len(clean_data)}\")\n",
    "    \n",
    "    if len(clean_data) < 5:\n",
    "        print(\"Not enough submissions.\")\n",
    "        continue\n",
    "\n",
    "    # --- STEP 4: FRIEDMAN TEST (On Median Ranks) ---\n",
    "    # We are now comparing the \"Consensus Rank\" of T1 vs T2 vs T3\n",
    "    \n",
    "    # Calculate mean of the median ranks for display\n",
    "    final_mean_ranks = clean_data.mean().sort_values(ascending=False)\n",
    "    \n",
    "    stat, p_value = stats.friedmanchisquare(*[clean_data[col] for col in clean_data.columns])\n",
    "    \n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"Friedman p-value: {p_value:.5f}\")\n",
    "    print(\"Mean of Median Ranks (Higher is better):\")\n",
    "    print(final_mean_ranks)\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n--- Post-Hoc: Nemenyi Test ---\")\n",
    "        \n",
    "        nemenyi = sp.posthoc_nemenyi_friedman(clean_data.reset_index(drop=True))\n",
    "        \n",
    "        # Filter for significant pairs to make reading easier\n",
    "        print(\"Significant Differences (p < 0.05):\")\n",
    "        cols = clean_data.columns\n",
    "        any_sig = False\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(i+1, len(cols)):\n",
    "                t1, t2 = cols[i], cols[j]\n",
    "                p_val = nemenyi.loc[t1, t2]\n",
    "                if p_val < 0.05:\n",
    "                    any_sig = True\n",
    "                    # Determine winner based on mean rank\n",
    "                    winner = t1 if final_mean_ranks[t1] > final_mean_ranks[t2] else t2\n",
    "                    loser = t2 if winner == t1 else t1\n",
    "                    print(f\"  * {winner} beats {loser} (p={p_val:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  - No significant difference between {t1} and {t2} (p={p_val:.4f})\")\n",
    "        \n",
    "        if not any_sig:\n",
    "            print(\"  (None found despite global significance)\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Result: No significant difference found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.groupby([\"feedback_type\", \"metric\"])[\"score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "evaluation_df[\"evaluator_type\"] = np.where(evaluation_df[\"expert_id\"] == \"llm\", \"llm\",\n",
    "    np.where(evaluation_df[\"Expert\"] == True, \"expert\", \"student\")\n",
    ")\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluator type (llm, vs student, vs expert)\n",
    "# llm if expert_id == \"llm\", expert if Expert == True, student otherwise\n",
    "\n",
    "evaluation_df.groupby([\"metric\", \"feedback_type\"])[\"score\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.groupby([\"feedback_type\"])[\"score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.groupby([\"metric\"])[\"score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Get unique submissions and create a mapping to consecutive numbers 1-100\n",
    "unique_submissions = sorted(evaluation_df['submission_id'].unique())\n",
    "submission_mapping = {sub_id: i+1 for i, sub_id in enumerate(unique_submissions)}\n",
    "\n",
    "# Add submission index to dataframe for plotting\n",
    "evaluation_df_plot = evaluation_df.copy()\n",
    "evaluation_df_plot['submission_index'] = evaluation_df_plot['submission_id'].map(submission_mapping)\n",
    "\n",
    "# Get unique metrics to assign different symbols and colors\n",
    "unique_metrics = evaluation_df['metric_id'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_metrics)))\n",
    "markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "\n",
    "# Create scatter plot for each metric\n",
    "for i, metric in enumerate(unique_metrics):\n",
    "    metric_data = evaluation_df_plot[evaluation_df_plot['metric_id'] == metric]\n",
    "    \n",
    "    ax.scatter(\n",
    "        metric_data['submission_index'], \n",
    "        metric_data['value'],\n",
    "        c=[colors[i]], \n",
    "        marker=markers[i % len(markers)],\n",
    "        label=f'Metric {i+1}',\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Submissions (1-100)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Punkte (Value)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Punktverteilung nach Submission und Metrik', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Set x-axis to show all submissions from 1 to 100\n",
    "ax.set_xlim(0, 101)\n",
    "ax.set_xticks(range(0, 101, 10))\n",
    "\n",
    "# Set y-axis to show all possible point values\n",
    "ax.set_ylim(-0.5, 5.5)\n",
    "ax.set_yticks(range(0, 6))\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Metriken')\n",
    "\n",
    "# Adjust layout to prevent legend cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show basic statistics\n",
    "print(f\"Anzahl Submissions: {len(unique_submissions)}\")\n",
    "print(f\"Anzahl Metriken: {len(unique_metrics)}\")\n",
    "print(f\"Punktebereich: {evaluation_df['value'].min()} - {evaluation_df['value'].max()}\")\n",
    "print(f\"Durchschnittliche Punkte: {evaluation_df['value'].mean():.2f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the unique values for metrics and feedback types\n",
    "print(\"Unique metric IDs:\")\n",
    "print(evaluation_df['metric_id'].unique())\n",
    "print(f\"\\nNumber of unique metrics: {evaluation_df['metric_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nUnique feedback types:\")\n",
    "print(evaluation_df['feedback_type'].unique())\n",
    "print(f\"\\nNumber of unique feedback types: {evaluation_df['feedback_type'].nunique()}\")\n",
    "\n",
    "# Check if we have readable names for metrics and feedback types\n",
    "print(\"\\nSample data with metric and feedback type info:\")\n",
    "print(evaluation_df[['metric_id', 'feedback_type', 'value']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what titles we have available\n",
    "print(\"Unique titles (metric names):\")\n",
    "print(evaluation_df['title'].unique())\n",
    "print(f\"\\nNumber of unique titles: {evaluation_df['title'].nunique()}\")\n",
    "\n",
    "# Calculate statistics for each metric (by title) and feedback type combination\n",
    "stats_list = []\n",
    "\n",
    "for title in evaluation_df['title'].unique():\n",
    "    for feedback_type in evaluation_df['feedback_type'].unique():\n",
    "        # Filter data for this combination\n",
    "        subset = evaluation_df[\n",
    "            (evaluation_df['title'] == title) & \n",
    "            (evaluation_df['feedback_type'] == feedback_type)\n",
    "        ]['value']\n",
    "        \n",
    "        if len(subset) > 0:  # Only calculate if we have data\n",
    "            stats = {\n",
    "                'metric_title': title,\n",
    "                'feedback_type': feedback_type,\n",
    "                'count': len(subset),\n",
    "                'mean': subset.mean(),\n",
    "                'median': subset.median(),\n",
    "                'std': subset.std(),\n",
    "                'min': subset.min(),\n",
    "                'max': subset.max(),\n",
    "                'q25': subset.quantile(0.25),\n",
    "                'q75': subset.quantile(0.75),\n",
    "                'iqr': subset.quantile(0.75) - subset.quantile(0.25)\n",
    "            }\n",
    "            stats_list.append(stats)\n",
    "\n",
    "# Create the statistics dataframe\n",
    "stats_df = pd.DataFrame(stats_list)\n",
    "\n",
    "# Round numerical values for better readability\n",
    "numerical_columns = ['mean', 'median', 'std', 'q25', 'q75', 'iqr']\n",
    "stats_df[numerical_columns] = stats_df[numerical_columns].round(3)\n",
    "\n",
    "print(\"\\nStatistics by Metric Title and Feedback Type:\")\n",
    "print(\"=\" * 80)\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# Also create a pivot table for easier comparison\n",
    "print(\"\\n\\nMean values pivot table:\")\n",
    "print(\"=\" * 50)\n",
    "mean_pivot = stats_df.pivot(index='metric_title', columns='feedback_type', values='mean')\n",
    "print(mean_pivot.round(3))\n",
    "\n",
    "print(\"\\n\\nStandard deviation pivot table:\")\n",
    "print(\"=\" * 50)\n",
    "std_pivot = stats_df.pivot(index='metric_title', columns='feedback_type', values='std')\n",
    "print(std_pivot.round(3))\n",
    "\n",
    "# Save the statistics dataframe\n",
    "print(f\"\\nStatistics dataframe shape: {stats_df.shape}\")\n",
    "print(\"Dataframe saved as 'stats_df' variable\")\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "stats_df.to_csv(\"data/4_expert_evaluation/stats_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second dataframe that also distinguishes between experts and non-experts\n",
    "print(\"Creating statistics dataframe with expert/non-expert distinction:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check the Expert column\n",
    "print(\"Expert status distribution:\")\n",
    "print(evaluation_df['Expert'].value_counts())\n",
    "print(f\"\\nExpert column type: {evaluation_df['Expert'].dtype}\")\n",
    "\n",
    "# Calculate statistics for each metric, feedback type, and expert status combination\n",
    "detailed_stats_list = []\n",
    "\n",
    "for title in evaluation_df['title'].unique():\n",
    "    for feedback_type in evaluation_df['feedback_type'].unique():\n",
    "        for expert_status in evaluation_df['Expert'].unique():\n",
    "            # Filter data for this combination\n",
    "            subset = evaluation_df[\n",
    "                (evaluation_df['title'] == title) & \n",
    "                (evaluation_df['feedback_type'] == feedback_type) &\n",
    "                (evaluation_df['Expert'] == expert_status)\n",
    "            ]['value']\n",
    "            \n",
    "            if len(subset) > 0:  # Only calculate if we have data\n",
    "                stats = {\n",
    "                    'metric_title': title,\n",
    "                    'feedback_type': feedback_type,\n",
    "                    'is_expert': expert_status,\n",
    "                    'count': len(subset),\n",
    "                    'mean': subset.mean(),\n",
    "                    'median': subset.median(),\n",
    "                    'std': subset.std(),\n",
    "                    'min': subset.min(),\n",
    "                    'max': subset.max(),\n",
    "                    'q25': subset.quantile(0.25),\n",
    "                    'q75': subset.quantile(0.75),\n",
    "                    'iqr': subset.quantile(0.75) - subset.quantile(0.25)\n",
    "                }\n",
    "                detailed_stats_list.append(stats)\n",
    "\n",
    "# Create the detailed statistics dataframe\n",
    "detailed_stats_df = pd.DataFrame(detailed_stats_list)\n",
    "\n",
    "# Round numerical values for better readability\n",
    "numerical_columns = ['mean', 'median', 'std', 'q25', 'q75', 'iqr']\n",
    "detailed_stats_df[numerical_columns] = detailed_stats_df[numerical_columns].round(3)\n",
    "\n",
    "print(\"\\nDetailed Statistics by Metric, Feedback Type, and Expert Status:\")\n",
    "print(\"=\" * 80)\n",
    "print(detailed_stats_df.to_string(index=False))\n",
    "\n",
    "# Create pivot tables for easier comparison\n",
    "print(\"\\n\\nMean values by Expert Status:\")\n",
    "print(\"=\" * 50)\n",
    "for expert_status in [False, True]:\n",
    "    expert_label = \"Expert\" if expert_status else \"Non-Expert\"\n",
    "    print(f\"\\n{expert_label}:\")\n",
    "    expert_data = detailed_stats_df[detailed_stats_df['is_expert'] == expert_status]\n",
    "    if len(expert_data) > 0:\n",
    "        expert_pivot = expert_data.pivot(index='metric_title', columns='feedback_type', values='mean')\n",
    "        print(expert_pivot.round(3))\n",
    "    else:\n",
    "        print(\"No data available\")\n",
    "\n",
    "# Save the detailed statistics dataframe\n",
    "print(f\"\\nDetailed statistics dataframe shape: {detailed_stats_df.shape}\")\n",
    "print(\"Detailed dataframe saved as 'detailed_stats_df' variable\")\n",
    "\n",
    "# Save to CSV\n",
    "detailed_stats_df.to_csv(\"data/4_expert_evaluation/detailed_stats_df.csv\", index=False)\n",
    "print(\"Saved to: data/4_expert_evaluation/detailed_stats_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots (whiskers diagrams) for the detailed statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create subplots for different comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Distribution of Evaluation Scores - Box Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Overall comparison by feedback type\n",
    "ax1 = axes[0, 0]\n",
    "feedback_types = evaluation_df['feedback_type'].unique()\n",
    "data_by_feedback = [evaluation_df[evaluation_df['feedback_type'] == ft]['value'].values for ft in feedback_types]\n",
    "box1 = ax1.boxplot(data_by_feedback, labels=feedback_types, patch_artist=True)\n",
    "ax1.set_title('Score Distribution by Feedback Type', fontweight='bold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, color in zip(box1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Plot 2: Comparison by expert status\n",
    "ax2 = axes[0, 1]\n",
    "expert_labels = ['Non-Expert', 'Expert']\n",
    "data_by_expert = [\n",
    "    evaluation_df[evaluation_df['Expert'] == False]['value'].values,\n",
    "    evaluation_df[evaluation_df['Expert'] == True]['value'].values\n",
    "]\n",
    "box2 = ax2.boxplot(data_by_expert, labels=expert_labels, patch_artist=True)\n",
    "ax2.set_title('Score Distribution by Expert Status', fontweight='bold')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Color the boxes\n",
    "expert_colors = ['lightyellow', 'lightpink']\n",
    "for patch, color in zip(box2['boxes'], expert_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Plot 3: Detailed comparison by feedback type and expert status\n",
    "ax3 = axes[1, 0]\n",
    "combined_labels = []\n",
    "combined_data = []\n",
    "\n",
    "for ft in feedback_types:\n",
    "    for expert in [False, True]:\n",
    "        expert_label = 'Expert' if expert else 'Non-Expert'\n",
    "        label = f\"{ft}\\n{expert_label}\"\n",
    "        data = evaluation_df[\n",
    "            (evaluation_df['feedback_type'] == ft) & \n",
    "            (evaluation_df['Expert'] == expert)\n",
    "        ]['value'].values\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            combined_labels.append(label)\n",
    "            combined_data.append(data)\n",
    "\n",
    "box3 = ax3.boxplot(combined_data, labels=combined_labels, patch_artist=True)\n",
    "ax3.set_title('Score Distribution by Feedback Type and Expert Status', fontweight='bold')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Color alternating boxes\n",
    "alt_colors = ['lightsteelblue', 'mistyrose'] * len(feedback_types)\n",
    "for patch, color in zip(box3['boxes'], alt_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Plot 4: Comparison by metrics (titles)\n",
    "ax4 = axes[1, 1]\n",
    "metric_titles = evaluation_df['title'].unique()\n",
    "data_by_metric = [evaluation_df[evaluation_df['title'] == title]['value'].values for title in metric_titles]\n",
    "\n",
    "# Shorten labels for better display\n",
    "short_labels = [title.split()[1] if len(title.split()) > 1 else title for title in metric_titles]\n",
    "box4 = ax4.boxplot(data_by_metric, labels=short_labels, patch_artist=True)\n",
    "ax4.set_title('Score Distribution by Metric', fontweight='bold')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Color the boxes\n",
    "metric_colors = ['lavender', 'lightcyan', 'wheat', 'honeydew']\n",
    "for patch, color in zip(box4['boxes'], metric_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for the box plots\n",
    "print(\"Summary Statistics for Box Plots:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. By Feedback Type:\")\n",
    "for ft in feedback_types:\n",
    "    data = evaluation_df[evaluation_df['feedback_type'] == ft]['value']\n",
    "    print(f\"{ft:>10}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}\")\n",
    "\n",
    "print(\"\\n2. By Expert Status:\")\n",
    "for expert, label in [(False, 'Non-Expert'), (True, 'Expert')]:\n",
    "    data = evaluation_df[evaluation_df['Expert'] == expert]['value']\n",
    "    print(f\"{label:>10}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}\")\n",
    "\n",
    "print(\"\\n3. By Metric:\")\n",
    "for title in metric_titles:\n",
    "    data = evaluation_df[evaluation_df['title'] == title]['value']\n",
    "    short_title = title.split()[1] if len(title.split()) > 1 else title\n",
    "    print(f\"{short_title:>12}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots with one box for each row of the detailed stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for each combination in detailed_stats_df\n",
    "box_data = []\n",
    "box_labels = []\n",
    "\n",
    "for idx, row in detailed_stats_df.iterrows():\n",
    "    # Filter data for this specific combination\n",
    "    subset = evaluation_df[\n",
    "        (evaluation_df['title'] == row['metric_title']) & \n",
    "        (evaluation_df['feedback_type'] == row['feedback_type']) &\n",
    "        (evaluation_df['Expert'] == row['is_expert'])\n",
    "    ]['value'].values\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        box_data.append(subset)\n",
    "        \n",
    "        # Create descriptive label\n",
    "        expert_label = 'Expert' if row['is_expert'] else 'Non-Expert'\n",
    "        metric_short = row['metric_title'].split()[1] if len(row['metric_title'].split()) > 1 else row['metric_title']\n",
    "        label = f\"{metric_short}\\n{row['feedback_type']}\\n{expert_label}\"\n",
    "        box_labels.append(label)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# Create box plot\n",
    "boxes = ax.boxplot(box_data, tick_labels=box_labels, patch_artist=True)\n",
    "\n",
    "# Color boxes based on feedback type\n",
    "colors = {'Cofee': 'lightblue', 'Tutor': 'lightgreen', 'LLM': 'lightcoral'}\n",
    "expert_colors = {'Expert': 0.8, 'Non-Expert': 0.5}  # Alpha values\n",
    "\n",
    "for i, (box, label) in enumerate(zip(boxes['boxes'], box_labels)):\n",
    "    # Extract feedback type and expert status from label\n",
    "    lines = label.split('\\n')\n",
    "    feedback_type = lines[1]\n",
    "    expert_status = lines[2]\n",
    "    \n",
    "    # Set color based on feedback type\n",
    "    base_color = colors.get(feedback_type, 'lightgray')\n",
    "    alpha = expert_colors.get(expert_status, 0.7)\n",
    "    \n",
    "    box.set_facecolor(base_color)\n",
    "    box.set_alpha(alpha)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Score Distribution for Each Detailed Statistics Combination\\n(24 combinations: 4 metrics × 3 feedback types × 2 expert levels)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Metric - Feedback Type - Expert Status', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Create custom legend for feedback types and expert status\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='lightblue', alpha=0.8, label='Cofee - Expert'),\n",
    "    Patch(facecolor='lightblue', alpha=0.5, label='Cofee - Non-Expert'),\n",
    "    Patch(facecolor='lightgreen', alpha=0.8, label='Tutor - Expert'),\n",
    "    Patch(facecolor='lightgreen', alpha=0.5, label='Tutor - Non-Expert'),\n",
    "    Patch(facecolor='lightcoral', alpha=0.8, label='LLM - Expert'),\n",
    "    Patch(facecolor='lightcoral', alpha=0.5, label='LLM - Non-Expert')\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the corresponding statistics for verification\n",
    "print(\"Detailed Statistics for Each Box:\")\n",
    "print(\"=\" * 60)\n",
    "print(detailed_stats_df[['metric_title', 'feedback_type', 'is_expert', 'count', 'mean', 'median', 'std']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate box plots for experts and non-experts\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create subplots for experts and non-experts\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('Score Distribution Comparison: Experts vs Non-Experts', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Define colors for feedback types\n",
    "colors = {'Cofee': 'lightblue', 'Tutor': 'lightgreen', 'LLM': 'lightcoral'}\n",
    "\n",
    "# Function to create box plot for a specific expert status\n",
    "def create_expert_plot(ax, is_expert, title_suffix):\n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    box_colors = []\n",
    "    \n",
    "    # Filter detailed stats for the specific expert status\n",
    "    expert_stats = detailed_stats_df[detailed_stats_df['is_expert'] == is_expert]\n",
    "    \n",
    "    for idx, row in expert_stats.iterrows():\n",
    "        # Filter data for this specific combination\n",
    "        subset = evaluation_df[\n",
    "            (evaluation_df['title'] == row['metric_title']) & \n",
    "            (evaluation_df['feedback_type'] == row['feedback_type']) &\n",
    "            (evaluation_df['Expert'] == row['is_expert'])\n",
    "        ]['value'].values\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            box_data.append(subset)\n",
    "            \n",
    "            # Create descriptive label (shorter for better readability)\n",
    "            metric_short = row['metric_title'].split()[1] if len(row['metric_title'].split()) > 1 else row['metric_title']\n",
    "            label = f\"{metric_short}\\n{row['feedback_type']}\"\n",
    "            box_labels.append(label)\n",
    "            box_colors.append(colors.get(row['feedback_type'], 'lightgray'))\n",
    "    \n",
    "    # Create box plot\n",
    "    boxes = ax.boxplot(box_data, tick_labels=box_labels, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for box, color in zip(boxes['boxes'], box_colors):\n",
    "        box.set_facecolor(color)\n",
    "        box.set_alpha(0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title(f'Score Distribution - {title_suffix}\\n(12 combinations: 4 metrics × 3 feedback types)', \n",
    "                 fontweight='bold', pad=15)\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_xlabel('Metric - Feedback Type', fontweight='bold')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    return expert_stats\n",
    "\n",
    "# Create plots for both groups\n",
    "non_expert_stats = create_expert_plot(ax1, False, 'Non-Experts')\n",
    "expert_stats = create_expert_plot(ax2, True, 'Experts')\n",
    "\n",
    "# Create shared legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='lightblue', alpha=0.7, label='Cofee'),\n",
    "    Patch(facecolor='lightgreen', alpha=0.7, label='Tutor'),\n",
    "    Patch(facecolor='lightcoral', alpha=0.7, label='LLM')\n",
    "]\n",
    "\n",
    "# Place legend outside the plots\n",
    "fig.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, 0.02), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for legend\n",
    "plt.show()\n",
    "\n",
    "# Print comparison statistics\n",
    "print(\"Comparison Statistics - Experts vs Non-Experts:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nNon-Expert Statistics:\")\n",
    "print(non_expert_stats[['metric_title', 'feedback_type', 'count', 'mean', 'median', 'std']].to_string(index=False))\n",
    "\n",
    "print(\"\\nExpert Statistics:\")\n",
    "print(expert_stats[['metric_title', 'feedback_type', 'count', 'mean', 'median', 'std']].to_string(index=False))\n",
    "\n",
    "# Calculate and show differences\n",
    "print(\"\\nMean Score Differences (Expert - Non-Expert):\")\n",
    "print(\"=\" * 50)\n",
    "for metric in evaluation_df['title'].unique():\n",
    "    for feedback in evaluation_df['feedback_type'].unique():\n",
    "        expert_mean = expert_stats[\n",
    "            (expert_stats['metric_title'] == metric) & \n",
    "            (expert_stats['feedback_type'] == feedback)\n",
    "        ]['mean'].iloc[0]\n",
    "        \n",
    "        non_expert_mean = non_expert_stats[\n",
    "            (non_expert_stats['metric_title'] == metric) & \n",
    "            (non_expert_stats['feedback_type'] == feedback)\n",
    "        ]['mean'].iloc[0]\n",
    "        \n",
    "        difference = expert_mean - non_expert_mean\n",
    "        metric_short = metric.split()[1] if len(metric.split()) > 1 else metric\n",
    "        print(f\"{metric_short:>12} - {feedback:>6}: {difference:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of complete evaluations for each submission\n",
    "print(\"Calculating complete evaluations per submission:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define what constitutes a \"complete\" evaluation\n",
    "# Complete = all 4 metrics × all 3 feedback types = 12 evaluations per submission\n",
    "total_metrics = evaluation_df['title'].nunique()\n",
    "total_feedback_types = evaluation_df['feedback_type'].nunique()\n",
    "expected_evaluations_per_submission = total_metrics * total_feedback_types\n",
    "\n",
    "print(f\"Expected evaluations per submission: {expected_evaluations_per_submission}\")\n",
    "print(f\"({total_metrics} metrics × {total_feedback_types} feedback types)\")\n",
    "\n",
    "# Group by submission and count unique combinations of metric and feedback type\n",
    "submission_completeness = []\n",
    "\n",
    "for submission_id in evaluation_df['submission_id'].unique():\n",
    "    # Get all evaluations for this submission\n",
    "    submission_data = evaluation_df[evaluation_df['submission_id'] == submission_id]\n",
    "    \n",
    "    # Count unique combinations of metric and feedback type\n",
    "    unique_combinations = submission_data.groupby(['title', 'feedback_type']).size()\n",
    "    num_complete_combinations = len(unique_combinations)\n",
    "    \n",
    "    # Calculate how many complete evaluation sets this submission has\n",
    "    # (some submissions might have multiple evaluators for the same combination)\n",
    "    total_evaluations = len(submission_data)\n",
    "    \n",
    "    # Count unique evaluators (expert_id) for this submission\n",
    "    unique_evaluators = submission_data['expert_id'].nunique()\n",
    "    \n",
    "    # Calculate completeness metrics\n",
    "    completeness_ratio = num_complete_combinations / expected_evaluations_per_submission\n",
    "    is_complete = num_complete_combinations == expected_evaluations_per_submission\n",
    "    \n",
    "    submission_completeness.append({\n",
    "        'submission_id': submission_id,\n",
    "        'total_evaluations': total_evaluations,\n",
    "        'unique_combinations': num_complete_combinations,\n",
    "        'unique_evaluators': unique_evaluators,\n",
    "        'expected_combinations': expected_evaluations_per_submission,\n",
    "        'completeness_ratio': completeness_ratio,\n",
    "        'is_complete': is_complete,\n",
    "        'avg_evaluations_per_combination': total_evaluations / num_complete_combinations if num_complete_combinations > 0 else 0\n",
    "    })\n",
    "\n",
    "# Create the completeness dataframe\n",
    "completeness_df = pd.DataFrame(submission_completeness)\n",
    "\n",
    "# Round numerical values\n",
    "completeness_df['completeness_ratio'] = completeness_df['completeness_ratio'].round(3)\n",
    "completeness_df['avg_evaluations_per_combination'] = completeness_df['avg_evaluations_per_combination'].round(2)\n",
    "\n",
    "# Sort by submission_id for better readability\n",
    "completeness_df = completeness_df.sort_values('submission_id').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nCompleteness Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total submissions analyzed: {len(completeness_df)}\")\n",
    "print(f\"Complete submissions (all 12 combinations): {completeness_df['is_complete'].sum()}\")\n",
    "print(f\"Incomplete submissions: {(~completeness_df['is_complete']).sum()}\")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"\\nEvaluation Count Statistics:\")\n",
    "print(f\"Minimum evaluations per submission: {completeness_df['total_evaluations'].min()}\")\n",
    "print(f\"Maximum evaluations per submission: {completeness_df['total_evaluations'].max()}\")\n",
    "print(f\"Average evaluations per submission: {completeness_df['total_evaluations'].mean():.1f}\")\n",
    "print(f\"Median evaluations per submission: {completeness_df['total_evaluations'].median():.1f}\")\n",
    "\n",
    "print(f\"\\nUnique Combinations Statistics:\")\n",
    "print(f\"Minimum unique combinations: {completeness_df['unique_combinations'].min()}\")\n",
    "print(f\"Maximum unique combinations: {completeness_df['unique_combinations'].max()}\")\n",
    "print(f\"Average unique combinations: {completeness_df['unique_combinations'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nEvaluator Statistics:\")\n",
    "print(f\"Minimum evaluators per submission: {completeness_df['unique_evaluators'].min()}\")\n",
    "print(f\"Maximum evaluators per submission: {completeness_df['unique_evaluators'].max()}\")\n",
    "print(f\"Average evaluators per submission: {completeness_df['unique_evaluators'].mean():.1f}\")\n",
    "\n",
    "# Show sample of the data\n",
    "print(f\"\\nSample of Completeness Data:\")\n",
    "print(\"=\" * 50)\n",
    "print(completeness_df.head(10).to_string(index=False))\n",
    "\n",
    "# Show incomplete submissions if any\n",
    "incomplete_submissions = completeness_df[~completeness_df['is_complete']]\n",
    "if len(incomplete_submissions) > 0:\n",
    "    print(f\"\\nIncomplete Submissions ({len(incomplete_submissions)} found):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(incomplete_submissions[['submission_id', 'unique_combinations', 'completeness_ratio']].to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"data/4_expert_evaluation/submission_completeness.csv\"\n",
    "completeness_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nCompleteness dataframe saved to: {csv_path}\")\n",
    "print(f\"Dataframe shape: {completeness_df.shape}\")\n",
    "print(\"Variables: 'completeness_df' contains the analysis results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
