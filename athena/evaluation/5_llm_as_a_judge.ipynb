{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## LLM Evaluation\n",
    "In this notebook, we will evaluate the feedback suggestions using an LLM-as-a-Judge. The goal is to compare the LLM's evaluation with the expert evaluation (step 4) and to analyze the differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We need to configure the LLM model and the evaluation metrics. The LLM model will be used to evaluate the feedback suggestions, while the metrics will define how we assess the quality of the feedback. Additionally, we will load the feedback suggestions (step 3) to be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Configuration\n",
    "Make sure to set the following environment variables in your `.env` file:\n",
    "- `LLM_EVALUATION_MODEL`: The name of the LLM model to use for evaluation.\n",
    "- `AZURE_OPENAI_API_KEY`: The API key for Azure OpenAI.\n",
    "- `AZURE_OPENAI_ENDPOINT`: The endpoint for Azure OpenAI.\n",
    "- `OPENAI_API_VERSION`: The API version for OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model_name = os.getenv(\"LLM_EVALUATION_MODEL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=model_name.replace(\"azure_openai_\", \"\"),\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=api_base,\n",
    "    api_version=api_version,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Define Metrics\n",
    "The metrics define how we assess the quality of the feedback. You can use the predefined metrics from the `metrics` file or reuse the metrics from the expert evaluation. If you want to compare the LLM's evaluation with the expert evaluation, make sure to use the same metrics as in step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "##### Define New Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts.metrics import (\n",
    "    completeness,\n",
    "    correctness,\n",
    "    actionability,\n",
    "    tone,\n",
    ")\n",
    "\n",
    "metrics = [completeness, correctness, actionability, tone]\n",
    "print(f\"Loaded metrics: {[metric.title for metric in metrics]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### Reuse Metrics from Expert Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from model.evaluation_model import Metric\n",
    "\n",
    "# Load Metrics from common evaluation config (created in 4_expert_evaluation.ipynb)\n",
    "config_path = \"data/4_expert_evaluation/output_depseudonymized/common_evaluation_config.json\"\n",
    "\n",
    "metrics = []\n",
    "with open(config_path, \"r\") as config_file:\n",
    "    common_evaluation_config = json.load(config_file)\n",
    "    metrics_config = common_evaluation_config.get(\"metrics\", [])\n",
    "    metrics = [Metric(**metric) for metric in metrics_config]\n",
    "\n",
    "print(f\"Loaded metrics: {[metric.title for metric in metrics]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Load Feedback Suggestions\n",
    "The feedback suggestions are stored in a CSV file (step 3). We will load the feedback suggestions and prepare them for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/3_feedback_suggestions/feedback_suggestions.csv\")\n",
    "\n",
    "print(f\"Feedback Types: {data[\"feedback_type\"].unique()}\")\n",
    "print(\n",
    "    f\"Exercises: {data[\"exercise_id\"].nunique()}, Submissions: {data[\"submission_id\"].nunique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Generate Prompts\n",
    "The prompts are generated based on the feedback suggestions and the metrics. The prompts will be used to evaluate the feedback suggestions using the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.llm_as_a_judge_service import generate_evaluation_requests\n",
    "\n",
    "# You can choose to evaluate only a specific feedback type by setting the filter (e.g. \"Tutor\")\n",
    "requests = generate_evaluation_requests(data, metrics, feedback_type_filter=None)\n",
    "\n",
    "print(f\"Number of requests: {len(requests)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Sample Prompts for Testing\n",
    "<mark>Optionally, you can sample a few prompts for testing purposes. This is useful to check if the prompts are generated correctly and to test the evaluation process without incurring high costs.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "requests = random.sample(requests, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Evaluate Feedback with LLM\n",
    "Evaluates the feedback suggestions using the LLM model. Saves the evaluations to a JSON file similar to the evaluation progress files from experts.\n",
    "\n",
    "The evaluation takes approximately one minute per 100 requests.\n",
    "\n",
    "<mark>**Note**: The evaluation using the LLM model incurs costs. Make sure to monitor your usage and costs. Try to use a small sample of prompts for testing before running the full evaluation. Try to run the full evaluation only once.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.llm_as_a_judge_service import process_feedback_evaluations\n",
    "\n",
    "output_path = \"data/5_llm_evaluation/\"\n",
    "\n",
    "process_feedback_evaluations(requests, output_path, model, metrics)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "evaluation-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
