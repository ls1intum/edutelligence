{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe508c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:52.230185Z",
     "start_time": "2025-03-24T15:32:51.988150Z"
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from athena.evaluation.service.json_service import group_exercise_data\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"data/3_sampled_submissions_with_feedback.csv\")\n",
    "feedback_types = data[\"feedback_type\"].unique()\n",
    "print(f\"Loaded data with feedback types: {feedback_types}\")\n",
    "\n",
    "number_of_exercises = data[\"exercise_id\"].nunique()\n",
    "print(f\"Number of exercises: {number_of_exercises}\")\n",
    "\n",
    "number_of_submissions = data[\"submission_id\"].nunique()\n",
    "print(f\"Number of submissions: {number_of_submissions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d994679624900f1",
   "metadata": {},
   "source": [
    "Define the metrics you want to use in the evaluation. You can also use the predefined metrics from the metrics file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c5a6ad345ab3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:52.297083Z",
     "start_time": "2025-03-24T15:32:52.234261Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from athena.evaluation.prompts.metrics import completeness, correctness, actionability, tone\n",
    "\n",
    "metrics = [completeness, correctness, actionability, tone]\n",
    "print(f\"Loaded metrics: {[metric.title for metric in metrics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2b824067adeaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:52.393312Z",
     "start_time": "2025-03-24T15:32:52.392076Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d4c9ed4e0a77d70",
   "metadata": {},
   "source": [
    "Test for a single submission + feedback type\n",
    "Look through the data for a suitable submission id. Then, select a feedback type to test the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7101a187d7061ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:53.295360Z",
     "start_time": "2025-03-24T15:32:52.399589Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model_name = os.getenv(\"LLM_EVALUATION_MODEL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    deployment_name=model_name.replace(\"azure_openai_\", \"\"),\n",
    "    openai_api_key=api_key,\n",
    "    azure_endpoint=api_base,\n",
    "    openai_api_version=api_version,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab528b52d593327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:53.361498Z",
     "start_time": "2025-03-24T15:32:53.302281Z"
    }
   },
   "outputs": [],
   "source": [
    "from athena.evaluation.service.llm_service import get_logprobs_langchain\n",
    "from athena.evaluation.prompts.llm_evaluation_prompt import get_formatted_prompt\n",
    "\n",
    "submission_id = 2506896\n",
    "feedback_type = \"Tutor\"\n",
    "\n",
    "submission_data = data[data[\"submission_id\"] == submission_id]\n",
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537dbde566d6d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:55.367527Z",
     "start_time": "2025-03-24T15:32:53.386851Z"
    }
   },
   "outputs": [],
   "source": [
    "exercise_data = group_exercise_data(submission_data, feedback_type)\n",
    "\n",
    "prompts = []\n",
    "for exercise in exercise_data:\n",
    "    print(f\"Exercise ID: {exercise.id}\")\n",
    "    for submission in exercise.submissions:\n",
    "        prompt = get_formatted_prompt(exercise, submission, submission.feedbacks, metrics)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "        # Nicely print the prompt\n",
    "        for message in prompt:\n",
    "            print(f\"--- {message.type.upper()} MESSAGE ---\")\n",
    "            print(message.content)\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "assessment = get_logprobs_langchain(prompts[0], model)\n",
    "print(assessment)\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427007e4e5bd479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:55.391748Z",
     "start_time": "2025-03-24T15:32:55.388569Z"
    }
   },
   "outputs": [],
   "source": [
    "assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae91277ab0fece8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:32:55.415854Z",
     "start_time": "2025-03-24T15:32:55.408223Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test = assessment.response.response_metadata.get(\"logprobs\")\n",
    "\n",
    "# Flatten the data\n",
    "flat_data = []\n",
    "token_index = 0\n",
    "for entry in test['content']:\n",
    "    # Basic token information\n",
    "    base_entry = {\n",
    "        'token_index': token_index,\n",
    "        'token': entry['token'],\n",
    "        'bytes': entry['bytes'],\n",
    "        'logprob': entry['logprob']\n",
    "    }\n",
    "\n",
    "    # Flatten top_logprobs\n",
    "    for top_logprob in entry['top_logprobs']:\n",
    "        flattened_entry = base_entry.copy()  # Copy base entry\n",
    "        flattened_entry.update({\n",
    "            'top_logprob_token': top_logprob['token'],\n",
    "            'top_logprob_bytes': top_logprob['bytes'],\n",
    "            'top_logprob_logprob': top_logprob['logprob']\n",
    "        })\n",
    "        flat_data.append(flattened_entry)\n",
    "\n",
    "    token_index += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flat_data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
