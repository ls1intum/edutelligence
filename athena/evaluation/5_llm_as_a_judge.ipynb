{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## LLM Evaluation\n",
    "In this notebook, we will evaluate the feedback suggestions using an LLM-as-a-Judge. The goal is to compare the LLM's evaluation with the expert evaluation (step 4) and to analyze the differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We need to configure the LLM model and the evaluation metrics. The LLM model will be used to evaluate the feedback suggestions, while the metrics will define how we assess the quality of the feedback. Additionally, we will load the feedback suggestions (step 3) to be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Configuration\n",
    "Make sure to set the following environment variables in your `.env` file:\n",
    "- `LLM_EVALUATION_MODEL`: The name of the LLM model to use for evaluation.\n",
    "- `AZURE_OPENAI_API_KEY`: The API key for Azure OpenAI.\n",
    "- `AZURE_OPENAI_ENDPOINT`: The endpoint for Azure OpenAI.\n",
    "- `OPENAI_API_VERSION`: The API version for OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model_name = os.getenv(\"LLM_EVALUATION_MODEL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    deployment_name=model_name.replace(\"azure_openai_\", \"\"),\n",
    "    openai_api_key=api_key,\n",
    "    azure_endpoint=api_base,\n",
    "    openai_api_version=api_version,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Define Metrics\n",
    "The metrics define how we assess the quality of the feedback. You can use the predefined metrics from the `metrics` file or define your own metrics. If you want to compare the LLM's evaluation with the expert evaluation, make sure to use the same metrics as in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts.metrics import (\n",
    "    completeness,\n",
    "    correctness,\n",
    "    actionability,\n",
    "    tone,\n",
    ")\n",
    "\n",
    "metrics = [completeness, correctness, actionability, tone]\n",
    "print(f\"Loaded metrics: {[metric.title for metric in metrics]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Load Feedback Suggestions\n",
    "The feedback suggestions are stored in a CSV file (step 3). We will load the feedback suggestions and prepare them for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/3_sampled_submissions_with_feedback.csv\")\n",
    "\n",
    "print(f\"Feedback Types: {data[\"feedback_type\"].unique()}\")\n",
    "print(\n",
    "    f\"Exercises: {data[\"exercise_id\"].nunique()}, Submissions: {data[\"submission_id\"].nunique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Generate Prompts\n",
    "The prompts are generated based on the feedback suggestions and the metrics. The prompts will be used to evaluate the feedback suggestions using the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.json_service import group_exercise_data\n",
    "from prompts.llm_evaluation_prompt import get_formatted_prompt\n",
    "\n",
    "prompts = []\n",
    "for feedback_type in data[\"feedback_type\"].unique():\n",
    "    exercises = group_exercise_data(data, feedback_type_filter=feedback_type)\n",
    "\n",
    "    for exercise in exercises:\n",
    "        for submission in exercise.submissions:\n",
    "            prompt = get_formatted_prompt(\n",
    "                exercise, submission, submission.feedbacks, metrics\n",
    "            )\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"submission_id\": submission.id,\n",
    "                    \"feedback_type\": feedback_type,\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(f\"Number of prompts: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Sample Prompts for Testing\n",
    "<mark>Optionally, you can sample a few prompts for testing purposes. This is useful to check if the prompts are generated correctly and to test the evaluation process without incurring high costs.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "prompts = random.sample(prompts, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Evaluate Feedback with LLM\n",
    "Evaluates the feedback suggestions using the LLM model. Saves the evaluations to a CSV file for further analysis.\n",
    "\n",
    "The evaluation takes approximately one second per prompt.\n",
    "\n",
    "<mark>**Note**: The evaluation using the LLM model incurs costs. Make sure to monitor your usage and costs. Try to use a small sample of prompts for testing before running the full evaluation. Try to run the full evaluation only once.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.llm_service import evaluate_feedback_with_model\n",
    "\n",
    "evaluations = []\n",
    "for entry in prompts:\n",
    "    evaluation = evaluate_feedback_with_model(\n",
    "        entry.get(\"prompt\"),\n",
    "        model,\n",
    "        entry.get(\"submission_id\"),\n",
    "        entry.get(\"feedback_type\"),\n",
    "    )\n",
    "    evaluations.append(evaluation)\n",
    "\n",
    "# Save the llm evaluations to a CSV file\n",
    "evaluations_df = pd.DataFrame(evaluations)\n",
    "evaluations_df.to_csv(\"data/5_llm_evaluation/llm_evaluations.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "evaluation-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
