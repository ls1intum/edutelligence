{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.json_service import load_evaluation_progress, load_common_evaluation_config\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "evaluation_progress_path = \"data/4_expert_evaluation/output_depseudonymized/\"\n",
    "llm_evaluation_progress_path = \"data/5_llm_evaluation/evaluation_progress_llm-as-a-judge.json\"\n",
    "\n",
    "evaluation_config_path = \"data/4_expert_evaluation/output_depseudonymized/common_evaluation_config.json\"\n",
    "\n",
    "participant_info_path = \"data/6_analysis/participant_info.csv\"\n",
    "\n",
    "analysis_output_path = \"data/6_analysis/\"\n",
    "\n",
    "# Load evaluation progress\n",
    "df = load_evaluation_progress(evaluation_progress_path, llm_evaluation_progress_path)\n",
    "\n",
    "# Load evaluation config\n",
    "evaluation_config_df = load_common_evaluation_config(evaluation_config_path)\n",
    "df = df.merge(evaluation_config_df, on=[\"exercise_id\", \"submission_id\", \"feedback_type\"], how=\"left\")\n",
    "\n",
    "# Load participant info and add LLM as a participant\n",
    "participant_info_df = pd.read_csv(participant_info_path, delimiter=\";\")\n",
    "participant_info_df = pd.concat([participant_info_df, pd.DataFrame([{\n",
    "    'expert_id': 'llm',\n",
    "    'evaluation_name': 'LLM as a judge',\n",
    "    'link': '',\n",
    "    'name': 'LLM',\n",
    "    'study_program': '',\n",
    "    'semester': pd.NA,\n",
    "    'eist_participation': pd.NA,\n",
    "    'pse_participation': pd.NA,\n",
    "    'tutoring_experience': pd.NA,\n",
    "    'group': 'LLM',\n",
    "}])])\n",
    "df = df.merge(participant_info_df, on=[\"expert_id\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['expert_id', 'study_program', 'semester', 'eist_participation', 'pse_participation', 'tutoring_experience', 'group', 'exercise_id', 'submission_id', 'feedback_type', 'metric', 'score', 'exercise', 'submission', 'feedback']]\n",
    "\n",
    "df.to_csv(os.path.join(analysis_output_path, \"evaluation_data_combined.csv\"), index=False)\n",
    "df = pd.read_csv(os.path.join(analysis_output_path, \"evaluation_data_combined.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set global plot style\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=['#105391', '#68A0C6', '#9BC6E8', '#999999', '#E07430', '#A1AC23', '#DAD7CC'])\n",
    "mpl.rcParams['font.family'] = 'Helvetica Neue'\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # Median Style (Thick Black Line)\n",
    "    'boxplot.medianprops.color': 'black',\n",
    "    'boxplot.medianprops.linewidth': 2.5,\n",
    "    \n",
    "    # Mean Style (White Triangle with Black Border)\n",
    "    'boxplot.meanprops.marker': '^',\n",
    "    'boxplot.meanprops.markerfacecolor': 'white',\n",
    "    'boxplot.meanprops.markeredgecolor': 'black',\n",
    "\n",
    "    # Legend Position (Lower Right)\n",
    "    'legend.loc': 'lower right',\n",
    "\n",
    "    # Figure Size\n",
    "    'figure.figsize': (6, 4),\n",
    "})\n",
    "\n",
    "metric_order = sorted(df['metric'].dropna().unique())\n",
    "feedback_type_order = ['Cofee', 'Tutor', 'LLM']\n",
    "group_order = ['Student', 'Instructor']\n",
    "\n",
    "# Filter 0 scores\n",
    "df = df[df['score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for group in [['Student'], ['Instructor'], ['LLM'], ['Student', 'Instructor']]:\n",
    "    df_group = df[df['group'].isin(group)]\n",
    "    plot_boxplot(\n",
    "        df = df_group,\n",
    "        x = 'feedback_type',\n",
    "        y = 'score',\n",
    "        hue = 'metric',\n",
    "        x_order = feedback_type_order,\n",
    "        hue_order = metric_order,\n",
    "        title = f'Assessment Scores by Feedback Type and Metric ({\"s & \".join(group)}s):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Metric',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{\"_\".join(group)}_feedback_type_metric_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for group in [['Student'], ['Instructor'], ['LLM'], ['Student', 'Instructor']]:\n",
    "    df_group = df[df['group'].isin(group)]\n",
    "    plot_boxplot(\n",
    "        df = df_group,\n",
    "        x = 'metric',\n",
    "        y = 'score',\n",
    "        hue = 'feedback_type',\n",
    "        x_order = metric_order,\n",
    "        hue_order = feedback_type_order,\n",
    "        title = f'Assessment Scores by Metric & Feedback Type ({\"s & \".join(group)}s):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Feedback Type',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{\"_\".join(group)}_metric_feedback_type_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for feedback_type in feedback_type_order:\n",
    "    df_feedback = df[df['feedback_type'] == feedback_type]\n",
    "    df_feedback = df_feedback[df_feedback['group'] != 'LLM']\n",
    "    plot_boxplot(\n",
    "        df = df_feedback,\n",
    "        x = 'metric',\n",
    "        y = 'score',\n",
    "        hue = 'group',\n",
    "        x_order = metric_order,\n",
    "        hue_order = group_order,\n",
    "        title = f'Assessment Scores by Metric & Feedback Source ({feedback_type}):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Group',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{\"_\".join(group)}_metric_group_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.plot_service import plot_boxplot\n",
    "\n",
    "for feedback_type in feedback_type_order:\n",
    "    df_feedback = df[df['feedback_type'] == feedback_type]\n",
    "    df_feedback = df_feedback[df_feedback['group'] != 'LLM']\n",
    "    plot_boxplot(\n",
    "        df = df_feedback,\n",
    "        x = 'group',\n",
    "        y = 'score',\n",
    "        hue = 'metric',\n",
    "        x_order = group_order,\n",
    "        hue_order = metric_order,\n",
    "        title = f'Assessment Scores by Feedback Source & Metric ({feedback_type}):',\n",
    "        ylabel = 'Score',\n",
    "        xlabel = '',\n",
    "        legend_title = 'Group',\n",
    "        plot_path = analysis_output_path,\n",
    "        filename = f'{feedback_type}_feedback_source_metric_boxplot.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP\n",
    "# ---------------------------------------------------------\n",
    "df = evaluation_df.copy()\n",
    "\n",
    "# df = df[df['Expert'] == True]  # Experts only\n",
    "# df = df[df['Expert'] == False]  # Students only\n",
    "# df = df[df['expert_id'] != \"llm\"] # Exclude LLM judge for this analysis\n",
    "df = df[df['expert_id'] == \"llm\"] # LLM judge only\n",
    "\n",
    "# Pre-processing\n",
    "df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "df['score'] = df['score'].replace(0, np.nan)\n",
    "\n",
    "unique_metrics = df['metric'].unique()\n",
    "\n",
    "print(f\"Total raw ratings: {len(df)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. ROBUST ANALYSIS LOOP (RANK-THEN-AGGREGATE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "for metric in unique_metrics:\n",
    "    print(f\"\\n\\n================ {metric.upper()} (SUBMISSION-LEVEL / RANK-BASED) ================\")\n",
    "    \n",
    "    # Filter by metric\n",
    "    df_metric = df[df['metric'] == metric]\n",
    "    \n",
    "    # --- STEP 1: PIVOT TO STUDENT LEVEL ---\n",
    "    # We need to see the student's side-by-side comparison first to calculate ranks.\n",
    "    student_level = df_metric.pivot_table(\n",
    "        index=['expert_id', 'submission_id'], # Unique session\n",
    "        columns='feedback_type', \n",
    "        values='score'\n",
    "    )\n",
    "    \n",
    "    # Remove incomplete sessions (Student didn't rate all 3 techniques)\n",
    "    # This is crucial for valid ranking.\n",
    "    student_level = student_level.dropna()\n",
    "    \n",
    "    print(f\"Valid Student Ratings (Raw Count): {len(student_level)}\")\n",
    "    \n",
    "    if len(student_level) < 5:\n",
    "        print(\"Not enough data to proceed.\")\n",
    "        continue\n",
    "\n",
    "    # --- STEP 2: CALCULATE RANKS (Per Student) ---\n",
    "    # This neutralizes the \"Grumpy vs Happy\" grader bias immediately.\n",
    "    # A '5' from a happy grader becomes Rank 3. A '2' from a grumpy grader also becomes Rank 3.\n",
    "    student_ranks = student_level.rank(axis=1, method='average', ascending=True)\n",
    "    \n",
    "    # --- STEP 3: AGGREGATE BY SUBMISSION (Median of Ranks) ---\n",
    "    # Now we combine the ranks for the same submission.\n",
    "    # We use groupby on the 'submission_id' level of the index.\n",
    "    submission_level = student_ranks.groupby(level='submission_id').median()\n",
    "    \n",
    "    # Drop submissions that might have become incomplete during aggregation \n",
    "    # (unlikely here given previous dropna, but good safety)\n",
    "    clean_data = submission_level.dropna()\n",
    "    \n",
    "    print(f\"Valid Submissions for Analysis: {len(clean_data)}\")\n",
    "    \n",
    "    if len(clean_data) < 5:\n",
    "        print(\"Not enough submissions.\")\n",
    "        continue\n",
    "\n",
    "    # --- STEP 4: FRIEDMAN TEST (On Median Ranks) ---\n",
    "    # We are now comparing the \"Consensus Rank\" of T1 vs T2 vs T3\n",
    "    \n",
    "    # Calculate mean of the median ranks for display\n",
    "    final_mean_ranks = clean_data.mean().sort_values(ascending=False)\n",
    "    \n",
    "    stat, p_value = stats.friedmanchisquare(*[clean_data[col] for col in clean_data.columns])\n",
    "    \n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"Friedman p-value: {p_value:.5f}\")\n",
    "    print(\"Mean of Median Ranks (Higher is better):\")\n",
    "    print(final_mean_ranks)\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n--- Post-Hoc: Nemenyi Test ---\")\n",
    "        \n",
    "        nemenyi = sp.posthoc_nemenyi_friedman(clean_data.reset_index(drop=True))\n",
    "        \n",
    "        # Filter for significant pairs to make reading easier\n",
    "        print(\"Significant Differences (p < 0.05):\")\n",
    "        cols = clean_data.columns\n",
    "        any_sig = False\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(i+1, len(cols)):\n",
    "                t1, t2 = cols[i], cols[j]\n",
    "                p_val = nemenyi.loc[t1, t2]\n",
    "                if p_val < 0.05:\n",
    "                    any_sig = True\n",
    "                    # Determine winner based on mean rank\n",
    "                    winner = t1 if final_mean_ranks[t1] > final_mean_ranks[t2] else t2\n",
    "                    loser = t2 if winner == t1 else t1\n",
    "                    print(f\"  * {winner} beats {loser} (p={p_val:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  - No significant difference between {t1} and {t2} (p={p_val:.4f})\")\n",
    "        \n",
    "        if not any_sig:\n",
    "            print(\"  (None found despite global significance)\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Result: No significant difference found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.groupby([\"feedback_type\", \"metric\"])[\"score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluator type (llm, vs student, vs expert)\n",
    "# llm if expert_id == \"llm\", expert if Expert == True, student otherwise\n",
    "\n",
    "evaluation_df.groupby([\"metric\", \"feedback_type\"])[\"score\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.groupby([\"feedback_type\"])[\"score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.groupby([\"metric\"])[\"score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Get unique submissions and create a mapping to consecutive numbers 1-100\n",
    "unique_submissions = sorted(evaluation_df['submission_id'].unique())\n",
    "submission_mapping = {sub_id: i+1 for i, sub_id in enumerate(unique_submissions)}\n",
    "\n",
    "# Add submission index to dataframe for plotting\n",
    "evaluation_df_plot = evaluation_df.copy()\n",
    "evaluation_df_plot['submission_index'] = evaluation_df_plot['submission_id'].map(submission_mapping)\n",
    "\n",
    "# Get unique metrics to assign different symbols and colors\n",
    "unique_metrics = evaluation_df['metric'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_metrics)))\n",
    "markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "\n",
    "# Create scatter plot for each metric\n",
    "for i, metric in enumerate(unique_metrics):\n",
    "    metric_data = evaluation_df_plot[evaluation_df_plot['metric'] == metric]\n",
    "    \n",
    "    ax.scatter(\n",
    "        metric_data['submission_index'], \n",
    "        metric_data['score'],\n",
    "        c=[colors[i]], \n",
    "        marker=markers[i % len(markers)],\n",
    "        label=f'Metric {i+1}',\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Submissions (1-100)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Punkte (Value)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Punktverteilung nach Submission und Metrik', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Set x-axis to show all submissions from 1 to 100\n",
    "ax.set_xlim(0, 101)\n",
    "ax.set_xticks(range(0, 101, 10))\n",
    "\n",
    "# Set y-axis to show all possible point values\n",
    "ax.set_ylim(-0.5, 5.5)\n",
    "ax.set_yticks(range(0, 6))\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Metriken')\n",
    "\n",
    "# Adjust layout to prevent legend cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show basic statistics\n",
    "print(f\"Anzahl Submissions: {len(unique_submissions)}\")\n",
    "print(f\"Anzahl Metriken: {len(unique_metrics)}\")\n",
    "print(f\"Punktebereich: {evaluation_df['score'].min()} - {evaluation_df['score'].max()}\")\n",
    "print(f\"Durchschnittliche Punkte: {evaluation_df['score'].mean():.2f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the unique values for metrics and feedback types\n",
    "print(\"Unique metric IDs:\")\n",
    "print(evaluation_df['metric'].unique())\n",
    "print(f\"\\nNumber of unique metrics: {evaluation_df['metric'].nunique()}\")\n",
    "\n",
    "print(\"\\nUnique feedback types:\")\n",
    "print(evaluation_df['feedback_type'].unique())\n",
    "print(f\"\\nNumber of unique feedback types: {evaluation_df['feedback_type'].nunique()}\")\n",
    "\n",
    "# Check if we have readable names for metrics and feedback types\n",
    "print(\"\\nSample data with metric and feedback type info:\")\n",
    "print(evaluation_df[['metric', 'feedback_type', 'score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what titles we have available\n",
    "print(\"Unique titles (metric names):\")\n",
    "print(evaluation_df['metric'].unique())\n",
    "print(f\"\\nNumber of unique titles: {evaluation_df['metric'].nunique()}\")\n",
    "\n",
    "# Calculate statistics for each metric (by title) and feedback type combination\n",
    "stats_list = []\n",
    "\n",
    "for title in evaluation_df['metric'].unique():\n",
    "    for feedback_type in evaluation_df['feedback_type'].unique():\n",
    "        # Filter data for this combination\n",
    "        subset = evaluation_df[\n",
    "            (evaluation_df['metric'] == title) & \n",
    "            (evaluation_df['feedback_type'] == feedback_type)\n",
    "        ]['score']\n",
    "        \n",
    "        if len(subset) > 0:  # Only calculate if we have data\n",
    "            stats = {\n",
    "                'metric_title': title,\n",
    "                'feedback_type': feedback_type,\n",
    "                'count': len(subset),\n",
    "                'mean': subset.mean(),\n",
    "                'median': subset.median(),\n",
    "                'std': subset.std(),\n",
    "                'min': subset.min(),\n",
    "                'max': subset.max(),\n",
    "                'q25': subset.quantile(0.25),\n",
    "                'q75': subset.quantile(0.75),\n",
    "                'iqr': subset.quantile(0.75) - subset.quantile(0.25)\n",
    "            }\n",
    "            stats_list.append(stats)\n",
    "\n",
    "# Create the statistics dataframe\n",
    "stats_df = pd.DataFrame(stats_list)\n",
    "\n",
    "# Round numerical values for better readability\n",
    "numerical_columns = ['mean', 'median', 'std', 'q25', 'q75', 'iqr']\n",
    "stats_df[numerical_columns] = stats_df[numerical_columns].round(3)\n",
    "\n",
    "print(\"\\nStatistics by Metric Title and Feedback Type:\")\n",
    "print(\"=\" * 80)\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# Also create a pivot table for easier comparison\n",
    "print(\"\\n\\nMean values pivot table:\")\n",
    "print(\"=\" * 50)\n",
    "mean_pivot = stats_df.pivot(index='metric_title', columns='feedback_type', values='mean')\n",
    "print(mean_pivot.round(3))\n",
    "\n",
    "print(\"\\n\\nStandard deviation pivot table:\")\n",
    "print(\"=\" * 50)\n",
    "std_pivot = stats_df.pivot(index='metric_title', columns='feedback_type', values='std')\n",
    "print(std_pivot.round(3))\n",
    "\n",
    "# Save the statistics dataframe\n",
    "print(f\"\\nStatistics dataframe shape: {stats_df.shape}\")\n",
    "print(\"Dataframe saved as 'stats_df' variable\")\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "stats_df.to_csv(\"data/4_expert_evaluation/stats_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second dataframe that also distinguishes between experts and non-experts\n",
    "print(\"Creating statistics dataframe with expert/non-expert distinction:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check the Expert column\n",
    "print(\"Expert status distribution:\")\n",
    "print(evaluation_df[evaluation_df['group'] == 'Expert'].value_counts())\n",
    "\n",
    "# Calculate statistics for each metric, feedback type, and expert status combination\n",
    "detailed_stats_list = []\n",
    "\n",
    "for title in evaluation_df['metric'].unique():\n",
    "    for feedback_type in evaluation_df['feedback_type'].unique():\n",
    "        for group in evaluation_df['group'].unique():\n",
    "            # Filter data for this combination\n",
    "            subset = evaluation_df[\n",
    "                (evaluation_df['metric'] == title) & \n",
    "                (evaluation_df['feedback_type'] == feedback_type) &\n",
    "                (evaluation_df['group'] == group)\n",
    "            ]['score']\n",
    "            \n",
    "            if len(subset) > 0:  # Only calculate if we have data\n",
    "                stats = {\n",
    "                    'metric_title': title,\n",
    "                    'feedback_type': feedback_type,\n",
    "                    'group': group,\n",
    "                    'count': len(subset),\n",
    "                    'mean': subset.mean(),\n",
    "                    'median': subset.median(),\n",
    "                    'std': subset.std(),\n",
    "                    'min': subset.min(),\n",
    "                    'max': subset.max(),\n",
    "                    'q25': subset.quantile(0.25),\n",
    "                    'q75': subset.quantile(0.75),\n",
    "                    'iqr': subset.quantile(0.75) - subset.quantile(0.25)\n",
    "                }\n",
    "                detailed_stats_list.append(stats)\n",
    "\n",
    "# Create the detailed statistics dataframe\n",
    "detailed_stats_df = pd.DataFrame(detailed_stats_list)\n",
    "\n",
    "# Round numerical values for better readability\n",
    "numerical_columns = ['mean', 'median', 'std', 'q25', 'q75', 'iqr']\n",
    "detailed_stats_df[numerical_columns] = detailed_stats_df[numerical_columns].round(3)\n",
    "\n",
    "print(\"\\nDetailed Statistics by Metric, Feedback Type, and Expert Status:\")\n",
    "print(\"=\" * 80)\n",
    "print(detailed_stats_df.to_string(index=False))\n",
    "\n",
    "# Create pivot tables for easier comparison\n",
    "print(\"\\n\\nMean values by Expert Status:\")\n",
    "print(\"=\" * 50)\n",
    "for group in ['Student', 'Expert', 'LLM']:\n",
    "    expert_data = detailed_stats_df[detailed_stats_df['group'] == group]\n",
    "    if len(expert_data) > 0:\n",
    "        expert_pivot = expert_data.pivot(index='metric_title', columns='feedback_type', values='mean')\n",
    "        print(expert_pivot.round(3))\n",
    "    else:\n",
    "        print(\"No data available\")\n",
    "\n",
    "# Save the detailed statistics dataframe\n",
    "print(f\"\\nDetailed statistics dataframe shape: {detailed_stats_df.shape}\")\n",
    "print(\"Detailed dataframe saved as 'detailed_stats_df' variable\")\n",
    "\n",
    "# Save to CSV\n",
    "detailed_stats_df.to_csv(\"data/4_expert_evaluation/detailed_stats_df.csv\", index=False)\n",
    "print(\"Saved to: data/4_expert_evaluation/detailed_stats_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots (whiskers diagrams) for the detailed statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create subplots for different comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Distribution of Evaluation Scores - Box Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Overall comparison by feedback type\n",
    "ax1 = axes[0, 0]\n",
    "feedback_types = evaluation_df['feedback_type'].unique()\n",
    "data_by_feedback = [evaluation_df[evaluation_df['feedback_type'] == ft]['score'].values for ft in feedback_types]\n",
    "box1 = ax1.boxplot(data_by_feedback, labels=feedback_types, patch_artist=True)\n",
    "ax1.set_title('Score Distribution by Feedback Type', fontweight='bold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, color in zip(box1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Plot 2: Comparison by expert status\n",
    "ax2 = axes[0, 1]\n",
    "expert_labels = ['Student', 'Instructor', 'LLM']\n",
    "data_by_expert = [\n",
    "    evaluation_df[evaluation_df['group'] == 'Student']['score'].values,\n",
    "    evaluation_df[evaluation_df['group'] == 'Instructor']['score'].values,\n",
    "    evaluation_df[evaluation_df['group'] == 'LLM']['score'].values\n",
    "]\n",
    "box2 = ax2.boxplot(data_by_expert, labels=expert_labels, patch_artist=True)\n",
    "ax2.set_title('Score Distribution by Expert Status', fontweight='bold')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Color the boxes\n",
    "expert_colors = ['lightyellow', 'lightpink', 'lightgray']\n",
    "for patch, color in zip(box2['boxes'], expert_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Plot 3: Detailed comparison by feedback type and expert status\n",
    "ax3 = axes[1, 0]\n",
    "combined_labels = []\n",
    "combined_data = []\n",
    "\n",
    "for ft in feedback_types:\n",
    "    for group in ['Student', 'Instructor', 'LLM']:\n",
    "        label = f\"{ft}\\n{group}\"\n",
    "        data = evaluation_df[\n",
    "            (evaluation_df['feedback_type'] == ft) & \n",
    "            (evaluation_df['group'] == group)\n",
    "        ]['score'].values\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            combined_labels.append(label)\n",
    "            combined_data.append(data)\n",
    "\n",
    "box3 = ax3.boxplot(combined_data, labels=combined_labels, patch_artist=True)\n",
    "ax3.set_title('Score Distribution by Feedback Type and Expert Status', fontweight='bold')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Color alternating boxes\n",
    "alt_colors = ['lightsteelblue', 'mistyrose'] * len(feedback_types)\n",
    "for patch, color in zip(box3['boxes'], alt_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Plot 4: Comparison by metrics (titles)\n",
    "ax4 = axes[1, 1]\n",
    "metric_titles = evaluation_df['metric'].unique()\n",
    "data_by_metric = [evaluation_df[evaluation_df['metric'] == title]['score'].values for title in metric_titles]\n",
    "\n",
    "# Shorten labels for better display\n",
    "short_labels = [title.split()[1] if len(title.split()) > 1 else title for title in metric_titles]\n",
    "box4 = ax4.boxplot(data_by_metric, labels=short_labels, patch_artist=True)\n",
    "ax4.set_title('Score Distribution by Metric', fontweight='bold')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Color the boxes\n",
    "metric_colors = ['lavender', 'lightcyan', 'wheat', 'honeydew']\n",
    "for patch, color in zip(box4['boxes'], metric_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for the box plots\n",
    "print(\"Summary Statistics for Box Plots:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. By Feedback Type:\")\n",
    "for ft in feedback_types:\n",
    "    data = evaluation_df[evaluation_df['feedback_type'] == ft]['score']\n",
    "    print(f\"{ft:>10}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}\")\n",
    "\n",
    "print(\"\\n2. By Expert Status:\")\n",
    "for group in ['Student', 'Instructor', 'LLM']:\n",
    "    data = evaluation_df[evaluation_df['group'] == group]['score']\n",
    "    print(f\"{group:>10}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}\")\n",
    "\n",
    "print(\"\\n3. By Metric:\")\n",
    "for title in metric_titles:\n",
    "    data = evaluation_df[evaluation_df['metric'] == title]['score']\n",
    "    short_title = title.split()[1] if len(title.split()) > 1 else title\n",
    "    print(f\"{short_title:>12}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots with one box for each row of the detailed stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for each combination in detailed_stats_df\n",
    "box_data = []\n",
    "box_labels = []\n",
    "\n",
    "for idx, row in detailed_stats_df.iterrows():\n",
    "    # Filter data for this specific combination\n",
    "    subset = evaluation_df[\n",
    "        (evaluation_df['metric'] == row['metric_title']) & \n",
    "        (evaluation_df['feedback_type'] == row['feedback_type']) &\n",
    "        (evaluation_df['group'] == row['group'])\n",
    "    ]['score'].values\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        box_data.append(subset)\n",
    "        \n",
    "        # Create descriptive label\n",
    "        metric_short = row['metric_title'].split()[1] if len(row['metric_title'].split()) > 1 else row['metric_title']\n",
    "        label = f\"{metric_short}\\n{row['feedback_type']}\\n{row['group']}\"\n",
    "        box_labels.append(label)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# Create box plot\n",
    "boxes = ax.boxplot(box_data, tick_labels=box_labels, patch_artist=True)\n",
    "\n",
    "# Color boxes based on feedback type\n",
    "colors = {'Cofee': 'lightblue', 'Tutor': 'lightgreen', 'LLM': 'lightcoral'}\n",
    "expert_colors = {'Expert': 0.8, 'Non-Expert': 0.5}  # Alpha values\n",
    "\n",
    "for i, (box, label) in enumerate(zip(boxes['boxes'], box_labels)):\n",
    "    # Extract feedback type and expert status from label\n",
    "    lines = label.split('\\n')\n",
    "    feedback_type = lines[1]\n",
    "    expert_status = lines[2]\n",
    "    \n",
    "    # Set color based on feedback type\n",
    "    base_color = colors.get(feedback_type, 'lightgray')\n",
    "    alpha = expert_colors.get(expert_status, 0.7)\n",
    "    \n",
    "    box.set_facecolor(base_color)\n",
    "    box.set_alpha(alpha)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Score Distribution for Each Detailed Statistics Combination\\n(24 combinations: 4 metrics × 3 feedback types × 2 expert levels)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Metric - Feedback Type - Expert Status', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Create custom legend for feedback types and expert status\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='lightblue', alpha=0.8, label='Cofee - Expert'),\n",
    "    Patch(facecolor='lightblue', alpha=0.5, label='Cofee - Non-Expert'),\n",
    "    Patch(facecolor='lightgreen', alpha=0.8, label='Tutor - Expert'),\n",
    "    Patch(facecolor='lightgreen', alpha=0.5, label='Tutor - Non-Expert'),\n",
    "    Patch(facecolor='lightcoral', alpha=0.8, label='LLM - Expert'),\n",
    "    Patch(facecolor='lightcoral', alpha=0.5, label='LLM - Non-Expert')\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the corresponding statistics for verification\n",
    "print(\"Detailed Statistics for Each Box:\")\n",
    "print(\"=\" * 60)\n",
    "print(detailed_stats_df[['metric_title', 'feedback_type', 'group', 'count', 'mean', 'median', 'std']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate box plots for experts and non-experts\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create subplots for experts and non-experts\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('Score Distribution Comparison: Experts vs Non-Experts', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Define colors for feedback types\n",
    "colors = {'Cofee': 'lightblue', 'Tutor': 'lightgreen', 'LLM': 'lightcoral'}\n",
    "\n",
    "# Function to create box plot for a specific expert status\n",
    "def create_expert_plot(ax, group):\n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    box_colors = []\n",
    "    \n",
    "    # Filter detailed stats for the specific expert status\n",
    "    expert_stats = detailed_stats_df[detailed_stats_df['group'] == group]\n",
    "    \n",
    "    for idx, row in expert_stats.iterrows():\n",
    "        # Filter data for this specific combination\n",
    "        subset = evaluation_df[\n",
    "            (evaluation_df['metric'] == row['metric_title']) & \n",
    "            (evaluation_df['feedback_type'] == row['feedback_type']) &\n",
    "            (evaluation_df['group'] == row['group'])\n",
    "        ]['score'].values\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            box_data.append(subset)\n",
    "            \n",
    "            # Create descriptive label (shorter for better readability)\n",
    "            metric_short = row['metric_title'].split()[1] if len(row['metric_title'].split()) > 1 else row['metric_title']\n",
    "            label = f\"{metric_short}\\n{row['feedback_type']}\"\n",
    "            box_labels.append(label)\n",
    "            box_colors.append(colors.get(row['feedback_type'], 'lightgray'))\n",
    "    \n",
    "    # Create box plot\n",
    "    boxes = ax.boxplot(box_data, tick_labels=box_labels, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for box, color in zip(boxes['boxes'], box_colors):\n",
    "        box.set_facecolor(color)\n",
    "        box.set_alpha(0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title(f'Score Distribution - {group}\\n(12 combinations: 4 metrics × 3 feedback types)', \n",
    "                 fontweight='bold', pad=15)\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_xlabel('Metric - Feedback Type', fontweight='bold')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    return expert_stats\n",
    "\n",
    "# Create plots for both groups\n",
    "non_expert_stats = create_expert_plot(ax1, 'Student')\n",
    "expert_stats = create_expert_plot(ax2, 'Instructor')\n",
    "\n",
    "# Create shared legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='lightblue', alpha=0.7, label='Cofee'),\n",
    "    Patch(facecolor='lightgreen', alpha=0.7, label='Tutor'),\n",
    "    Patch(facecolor='lightcoral', alpha=0.7, label='LLM')\n",
    "]\n",
    "\n",
    "# Place legend outside the plots\n",
    "fig.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, 0.02), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for legend\n",
    "plt.show()\n",
    "\n",
    "# Print comparison statistics\n",
    "print(\"Comparison Statistics - Experts vs Non-Experts:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nNon-Expert Statistics:\")\n",
    "print(non_expert_stats[['metric_title', 'feedback_type', 'count', 'mean', 'median', 'std']].to_string(index=False))\n",
    "\n",
    "print(\"\\nExpert Statistics:\")\n",
    "print(expert_stats[['metric_title', 'feedback_type', 'count', 'mean', 'median', 'std']].to_string(index=False))\n",
    "\n",
    "# Calculate and show differences\n",
    "print(\"\\nMean Score Differences (Expert - Non-Expert):\")\n",
    "print(\"=\" * 50)\n",
    "for metric in evaluation_df['metric'].unique():\n",
    "    for feedback in evaluation_df['feedback_type'].unique():\n",
    "        expert_mean = expert_stats[\n",
    "            (expert_stats['metric_title'] == metric) & \n",
    "            (expert_stats['feedback_type'] == feedback)\n",
    "        ]['mean'].iloc[0]\n",
    "        \n",
    "        non_expert_mean = non_expert_stats[\n",
    "            (non_expert_stats['metric_title'] == metric) & \n",
    "            (non_expert_stats['feedback_type'] == feedback)\n",
    "        ]['mean'].iloc[0]\n",
    "        \n",
    "        difference = expert_mean - non_expert_mean\n",
    "        metric_short = metric.split()[1] if len(metric.split()) > 1 else metric\n",
    "        print(f\"{metric_short:>12} - {feedback:>6}: {difference:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of complete evaluations for each submission\n",
    "print(\"Calculating complete evaluations per submission:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define what constitutes a \"complete\" evaluation\n",
    "# Complete = all 4 metrics × all 3 feedback types = 12 evaluations per submission\n",
    "total_metrics = evaluation_df['metric'].nunique()\n",
    "total_feedback_types = evaluation_df['feedback_type'].nunique()\n",
    "expected_evaluations_per_submission = total_metrics * total_feedback_types\n",
    "\n",
    "print(f\"Expected evaluations per submission: {expected_evaluations_per_submission}\")\n",
    "print(f\"({total_metrics} metrics × {total_feedback_types} feedback types)\")\n",
    "\n",
    "# Group by submission and count unique combinations of metric and feedback type\n",
    "submission_completeness = []\n",
    "\n",
    "for submission_id in evaluation_df['submission_id'].unique():\n",
    "    # Get all evaluations for this submission\n",
    "    submission_data = evaluation_df[evaluation_df['submission_id'] == submission_id]\n",
    "    \n",
    "    # Count unique combinations of metric and feedback type\n",
    "    unique_combinations = submission_data.groupby(['metric', 'feedback_type']).size()\n",
    "    num_complete_combinations = len(unique_combinations)\n",
    "    \n",
    "    # Calculate how many complete evaluation sets this submission has\n",
    "    # (some submissions might have multiple evaluators for the same combination)\n",
    "    total_evaluations = len(submission_data)\n",
    "    \n",
    "    # Count unique evaluators (expert_id) for this submission\n",
    "    unique_evaluators = submission_data['expert_id'].nunique()\n",
    "    \n",
    "    # Calculate completeness metrics\n",
    "    completeness_ratio = num_complete_combinations / expected_evaluations_per_submission\n",
    "    is_complete = num_complete_combinations == expected_evaluations_per_submission\n",
    "    \n",
    "    submission_completeness.append({\n",
    "        'submission_id': submission_id,\n",
    "        'total_evaluations': total_evaluations,\n",
    "        'unique_combinations': num_complete_combinations,\n",
    "        'unique_evaluators': unique_evaluators,\n",
    "        'expected_combinations': expected_evaluations_per_submission,\n",
    "        'completeness_ratio': completeness_ratio,\n",
    "        'is_complete': is_complete,\n",
    "        'avg_evaluations_per_combination': total_evaluations / num_complete_combinations if num_complete_combinations > 0 else 0\n",
    "    })\n",
    "\n",
    "# Create the completeness dataframe\n",
    "completeness_df = pd.DataFrame(submission_completeness)\n",
    "\n",
    "# Round numerical values\n",
    "completeness_df['completeness_ratio'] = completeness_df['completeness_ratio'].round(3)\n",
    "completeness_df['avg_evaluations_per_combination'] = completeness_df['avg_evaluations_per_combination'].round(2)\n",
    "\n",
    "# Sort by submission_id for better readability\n",
    "completeness_df = completeness_df.sort_values('submission_id').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nCompleteness Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total submissions analyzed: {len(completeness_df)}\")\n",
    "print(f\"Complete submissions (all 12 combinations): {completeness_df['is_complete'].sum()}\")\n",
    "print(f\"Incomplete submissions: {(~completeness_df['is_complete']).sum()}\")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"\\nEvaluation Count Statistics:\")\n",
    "print(f\"Minimum evaluations per submission: {completeness_df['total_evaluations'].min()}\")\n",
    "print(f\"Maximum evaluations per submission: {completeness_df['total_evaluations'].max()}\")\n",
    "print(f\"Average evaluations per submission: {completeness_df['total_evaluations'].mean():.1f}\")\n",
    "print(f\"Median evaluations per submission: {completeness_df['total_evaluations'].median():.1f}\")\n",
    "\n",
    "print(f\"\\nUnique Combinations Statistics:\")\n",
    "print(f\"Minimum unique combinations: {completeness_df['unique_combinations'].min()}\")\n",
    "print(f\"Maximum unique combinations: {completeness_df['unique_combinations'].max()}\")\n",
    "print(f\"Average unique combinations: {completeness_df['unique_combinations'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nEvaluator Statistics:\")\n",
    "print(f\"Minimum evaluators per submission: {completeness_df['unique_evaluators'].min()}\")\n",
    "print(f\"Maximum evaluators per submission: {completeness_df['unique_evaluators'].max()}\")\n",
    "print(f\"Average evaluators per submission: {completeness_df['unique_evaluators'].mean():.1f}\")\n",
    "\n",
    "# Show sample of the data\n",
    "print(f\"\\nSample of Completeness Data:\")\n",
    "print(\"=\" * 50)\n",
    "print(completeness_df.head(10).to_string(index=False))\n",
    "\n",
    "# Show incomplete submissions if any\n",
    "incomplete_submissions = completeness_df[~completeness_df['is_complete']]\n",
    "if len(incomplete_submissions) > 0:\n",
    "    print(f\"\\nIncomplete Submissions ({len(incomplete_submissions)} found):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(incomplete_submissions[['submission_id', 'unique_combinations', 'completeness_ratio']].to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"data/4_expert_evaluation/submission_completeness.csv\"\n",
    "completeness_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nCompleteness dataframe saved to: {csv_path}\")\n",
    "print(f\"Dataframe shape: {completeness_df.shape}\")\n",
    "print(\"Variables: 'completeness_df' contains the analysis results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
