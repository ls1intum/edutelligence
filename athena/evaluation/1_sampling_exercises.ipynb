{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcca46b0c9105c1",
   "metadata": {},
   "source": [
    "## Get the data from the database\n",
    "Sample the exercise ids manually from the available exercises and adapt the `EXERCISE_IDS` variable accordingly.\n",
    "The `fetch_data_from_db` function fetches the data from the database for the specified exercise IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12cb557da6fe73",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from athena.evaluation.service.db_service import fetch_data_from_db\n",
    "from langid import classify\n",
    "\n",
    "EXERCISE_IDS = {4066, 642, 544, 506}\n",
    "data = fetch_data_from_db(EXERCISE_IDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db9f04aba98c9a",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The data preprocessing steps include:\n",
    "- Dropping rows with missing or invalid data.\n",
    "- Filtering out non-English submissions.\n",
    "\n",
    "You can adapt the data preprocessing steps based on the requirements of your evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba6aaa05d7c45c",
   "metadata": {},
   "source": [
    "### Drop Rows with Missing or Invalid Data\n",
    "Drops the rows with missing data in the `submission_text` and `result_score` columns. Also, filters out submissions with empty text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18b450ab913621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:00:49.860039Z",
     "start_time": "2025-03-24T15:52:30.970638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing or invalid data\n",
    "data = data.dropna(subset=[\"submission_text\", \"result_score\"])\n",
    "data = data[data[\"submission_text\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263edf1b4923d42a",
   "metadata": {},
   "source": [
    "### Filter Out Non-English Submissions\n",
    "Filters out non-English submissions using the `langid` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e1a2257ffc0d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:00:49.865799Z",
     "start_time": "2025-03-24T15:52:30.994267Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_texts = data[\"submission_text\"].unique()\n",
    "classification_results = {text: classify(text)[0] == \"en\" for text in unique_texts}\n",
    "\n",
    "data[\"is_english\"] = data[\"submission_text\"].map(classification_results)\n",
    "data = data[data[\"is_english\"]]\n",
    "\n",
    "data = data.drop(columns=[\"is_english\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378102b58be762b",
   "metadata": {},
   "source": [
    "## Save the Sampled Exercises in a CSV File\n",
    "Save the sampled exercises to a CSV file for the next steps in the evaluation process.\n",
    "You can also retrieve the sampled exercises from an existing CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff3303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:00:49.866376Z",
     "start_time": "2025-03-24T15:52:35.529155Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/1_sampled_exercises.csv\", index=False)\n",
    "# data = pd.read_csv(\"../data/1_sampled_exercises.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df914573f544333",
   "metadata": {},
   "source": [
    "## Examples of Analysing the Sampled Exercises\n",
    "The following examples demonstrate some basic analysis of the sampled exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efdcda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:00:49.866738Z",
     "start_time": "2025-03-24T15:52:36.116002Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_submissions = data[\"submission_id\"].nunique()\n",
    "print(f\"Overall number of submissions: {overall_submissions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da6e6aaaec780d",
   "metadata": {},
   "source": [
    "Creates a grouped DataFrame to count the number of distinct feedback IDs, submissions, and total feedbacks per score.\n",
    "Saves the data to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6f3ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:00:49.867311Z",
     "start_time": "2025-03-24T15:52:36.133506Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped_data = (\n",
    "    data\n",
    "    .groupby([\"exercise_id\", \"result_score\"])\n",
    "    .agg(\n",
    "        distinct_feedback_count=(\"feedback_id\", \"nunique\"),  # Count distinct feedback IDs per score\n",
    "        submission_count=(\"submission_id\", \"nunique\"),       # Count distinct submissions per score\n",
    "        feedback_count=(\"feedback_id\", \"nunique\")            # Total feedbacks per score\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "total_feedbacks_per_exercise = (\n",
    "    data\n",
    "    .groupby(\"exercise_id\")[\"feedback_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"feedback_id\": \"total_feedback_count\"})\n",
    ")\n",
    "\n",
    "total_submissions_per_exercise = (\n",
    "    data\n",
    "    .groupby(\"exercise_id\")[\"submission_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"submission_id\": \"total_submission_count\"})\n",
    ")\n",
    "\n",
    "# Merge the total feedback count and total submission count back into the grouped data\n",
    "grouped_data = grouped_data.merge(total_feedbacks_per_exercise, on=\"exercise_id\")\n",
    "grouped_data = grouped_data.merge(total_submissions_per_exercise, on=\"exercise_id\")\n",
    "\n",
    "# Calculate average number of feedbacks per exercise and score\n",
    "grouped_data[\"avg_feedbacks_per_score\"] = (\n",
    "    grouped_data[\"feedback_count\"] / grouped_data[\"submission_count\"]\n",
    ")\n",
    "\n",
    "grouped_data = grouped_data[[\n",
    "    \"exercise_id\",\n",
    "    \"result_score\",\n",
    "    \"submission_count\",\n",
    "    \"total_submission_count\",\n",
    "    \"total_feedback_count\",\n",
    "    \"feedback_count\",\n",
    "    \"avg_feedbacks_per_score\"\n",
    "]]\n",
    "\n",
    "grouped_data.to_csv(\"../data/grouped_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804e2ae2a836e36",
   "metadata": {},
   "source": [
    "Visualize the relationship between the scores and the average number of feedbacks per score using the grouped data from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888612b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:00:49.867684Z",
     "start_time": "2025-03-24T15:52:36.160491Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the grouped data\n",
    "grouped_data = pd.read_csv(\"data/grouped_data.csv\")\n",
    "\n",
    "# Create a color and marker map for exercises\n",
    "exercise_ids = grouped_data[\"exercise_id\"].unique()\n",
    "colors = plt.cm.tab10(range(len(exercise_ids)))  # Use a colormap for distinct colors\n",
    "markers = ['o', 's', 'D', '^', 'v', 'P', '*', 'X']  # Different marker styles\n",
    "marker_map = {exercise_id: markers[i % len(markers)] for i, exercise_id in enumerate(exercise_ids)}\n",
    "color_map = {exercise_id: colors[i] for i, exercise_id in enumerate(exercise_ids)}\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for exercise_id in exercise_ids:\n",
    "    subset = grouped_data[grouped_data[\"exercise_id\"] == exercise_id]\n",
    "    x = subset[\"avg_feedbacks_per_score\"]\n",
    "    y = subset[\"result_score\"]\n",
    "\n",
    "    # Scatter points\n",
    "    plt.scatter(\n",
    "        x, y,\n",
    "        label=f\"Exercise {exercise_id}\",\n",
    "        color=color_map[exercise_id],\n",
    "        marker=marker_map[exercise_id],\n",
    "        s=100,  # Marker size\n",
    "        alpha=0.7  # Transparency\n",
    "    )\n",
    "\n",
    "    # Compute regression line\n",
    "    if len(subset) > 1:  # Regression is meaningful only if there are multiple points\n",
    "        coefficients = np.polyfit(x, y, 1)  # Linear regression (degree=1)\n",
    "        regression_line = np.poly1d(coefficients)\n",
    "        plt.plot(\n",
    "            x, regression_line(x),\n",
    "            color=color_map[exercise_id],\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Average Number of Feedbacks\", fontsize=12)\n",
    "plt.ylabel(\"Scores\", fontsize=12)\n",
    "plt.title(\"Scores vs. Average Number of Feedbacks\", fontsize=14)\n",
    "plt.legend(title=\"Exercises\", loc=\"upper left\", fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
